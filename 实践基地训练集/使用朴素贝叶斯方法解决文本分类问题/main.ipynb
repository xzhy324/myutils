{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务介绍\n",
    "\n",
    "**任务：本次实践使用朴素贝叶斯方法解决文本分类问题**\n",
    "\n",
    "**实践平台：百度AI实训平台-AI Studio、python3.7**\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3f746f16e75242c5b176e93c93b3122ed67a13057b2f4b99811b03c95a745723)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data43470  data6826\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集介绍\n",
    "网上公开中文新闻数据：\n",
    "\n",
    "数据来源：从中文新闻网站上爬取56821条新闻摘要数据。\n",
    "\n",
    "数据内容：数据集中包含10个类别\n",
    "\n",
    "数据划分：本次实践将其中90%作为训练集，10%作为验证集。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/f1b298fb982b4cec9a3ee66b8d0b67d497995f140de142ec9a459914eace6931)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型选择\n",
    "**贝叶斯分类：** 贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。\n",
    "\n",
    "**贝叶斯公式:**    $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "\n",
    "在文本分类（classification）问题中，我们要将一个句子分到某个类别，我们将句子中的词或字视为句子的属性，因此一个句子是由个属性（字/词）组成的，把众多属性看做一个向量，即X=(x1,x2,x3,…,xn)，用X这个向量来代表这个句子。\n",
    "类别也有很多种，我们用集合Y={y1,y2,…ym}表示。\n",
    "\n",
    "一个句子属于yk类别的概率可以表示为：$P(Y=yk|X=(x1,x2,x3,…,xn))$\n",
    "\n",
    "如果一个句子的属性向量X属于yk类别的概率最大:\n",
    "\n",
    "即,  $P(Y=yk|X) = max{P(Y=y1|X),P(Y=y2|X),P(Y=y3|X),...,P(Y=ym|X)}$,\n",
    "\n",
    "其中，X=(x1,x2,x3,…,xn)\n",
    "\n",
    "就可以给X打上yk标签，意思是说X属于yk类别。这就是所谓的分类(Classification)。\n",
    "\n",
    "**朴素贝叶斯：**\n",
    "\n",
    "假设X=(x1,x2,x3,…,xn)中的所有属性都是独立的，\n",
    "\n",
    "即$P(Y=yk|X=(x1,x2,x3,…,xn)) = \\frac{P(x1｜Y=yk)P(x2｜Y=yk)...P(xn｜Y=yk)P(Y=yk)}{P(x1)P(x2)...P(xn)}$\n",
    "\n",
    "**拉普拉斯平滑的引入：**\n",
    "\n",
    "如果某个属性的条件概率为0，则会导致整体概率为零，为了避免这种情况出现，引入拉普拉斯平滑参数，即将条件概率为0的属性的概率设定为固定值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整体流程\n",
    "1、数据准备：\n",
    "* 数据预处理：对去除文本中的标点符号，并对句子进行分词。\n",
    "* 生成词典：一个词即为一个特征，统计所有句子中出现过的词，去除停用词，并统计词频，形成一个词典。\n",
    "* 确定特征词：从词典中选择一部分词作为特征词。\n",
    "* 形成特征向量：利用特征词，将每一个句子转化为特征向量。\n",
    "\n",
    "2、训练分类器模型\n",
    "\n",
    "3、评估训练效果\n",
    "\n",
    "4、使用模型进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**关于特征词和特征向量：**\n",
    "\n",
    "**【举例说明】**\n",
    "\n",
    "停用词：是、的、你、我、他，这、那\n",
    "\n",
    "特征词：[‘中国’，’西安‘，’天安门‘，’首都‘，’故宫’，‘机器学习’，’北京‘]\n",
    "\n",
    "text：北京是中国的首都\n",
    "\n",
    "通过分词之后 [北京，是，中国，的，首都]\n",
    "\n",
    "去除停用词：[北京，中国，首都]\n",
    "\n",
    "形成特征向量：[1,0,0,1,0,0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#导入必要的包\n",
    "import random\n",
    "import jieba  # 处理中文\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import re,string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def text_to_words(file_path):\n",
    "    '''\n",
    "    分词\n",
    "    return:sentences_arr, lab_arr\n",
    "    '''\n",
    "    sentences_arr = []\n",
    "    lab_arr = []\n",
    "    with open(file_path,'r',encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            lab_arr.append(line.split('_!_')[1])\n",
    "            sentence = line.split('_!_')[-1].strip()\n",
    "            sentence = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\')]+|[+——()?【】“”！，。？、~@#￥%……&*（）《》：]+\", \"\",sentence) #去除标点符号\n",
    "            sentence = jieba.lcut(sentence, cut_all=False)\n",
    "            sentences_arr.append(sentence)\n",
    "    return sentences_arr, lab_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_stopwords(file_path):\n",
    "    '''\n",
    "    创建停用词表\n",
    "    参数 file_path:停用词文本路径\n",
    "    return：停用词list\n",
    "    '''\n",
    "    stopwords = [line.strip() for line in open(file_path, encoding='UTF-8').readlines()]\n",
    "    return stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_dict(sentences_arr,stopswords):\n",
    "    '''\n",
    "    遍历数据，去除停用词，统计词频\n",
    "    return: 生成词典\n",
    "    '''\n",
    "    word_dic = {}\n",
    "    for sentence in sentences_arr:\n",
    "        for word in sentence:\n",
    "            if word != ' ' and word.isalpha():\n",
    "                if word not in stopswords:\n",
    "                    word_dic[word] = word_dic.get(word,1) + 1\n",
    "    word_dic=sorted(word_dic.items(),key=lambda x:x[1],reverse=True) #按词频序排列\n",
    "\n",
    "    return word_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_words(word_dic,word_num):\n",
    "    '''\n",
    "    从词典中选取N个特征词，形成特征词列表\n",
    "    return: 特征词列表\n",
    "    '''\n",
    "    n = 0\n",
    "    feature_words = []\n",
    "    for word in word_dic:\n",
    "        if n < word_num:\n",
    "            feature_words.append(word[0])\n",
    "        n += 1\n",
    "    return feature_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 文本特征\n",
    "def get_text_features(train_data_list, test_data_list, feature_words):\n",
    "    '''\n",
    "    根据特征词，将数据集中的句子转化为特征向量\n",
    "    '''\n",
    "    def text_features(text, feature_words):\n",
    "        text_words = set(text)\n",
    "        features = [1 if word in text_words else 0 for word in feature_words] # 形成特征向量\n",
    "        return features\n",
    "    train_feature_list = [text_features(text, feature_words) for text in train_data_list]\n",
    "    test_feature_list = [text_features(text, feature_words) for text in test_data_list]\n",
    "    return train_feature_list, test_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.812 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#获取分词后的数据及标签\n",
    "sentences_arr, lab_arr = text_to_words('data/data6826/news_classify_data.txt')\n",
    "#加载停用词\n",
    "stopwords = load_stopwords('data/data43470/stopwords_cn.txt')\n",
    "# 生成词典\n",
    "word_dic = get_dict(sentences_arr,stopwords)\n",
    "#数据集划分\n",
    "train_data_list, test_data_list, train_class_list, test_class_list = model_selection.train_test_split(sentences_arr, \n",
    "                                                                                                      lab_arr, \n",
    "                                                                                                      test_size=0.1)\n",
    "#生成特征词列表\n",
    "feature_words =  get_feature_words(word_dic,10000)\n",
    "\n",
    "#生成特征向量\n",
    "train_feature_list,test_feature_list = get_text_features(train_data_list,test_data_list,feature_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.7700\n",
      "Classification report for classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.70      0.72       522\n",
      "           1       0.74      0.86      0.79       558\n",
      "           2       0.89      0.82      0.86       504\n",
      "           3       0.64      0.66      0.65       784\n",
      "           4       0.82      0.79      0.81       371\n",
      "           5       0.85      0.85      0.85       733\n",
      "           6       0.82      0.83      0.83       847\n",
      "           7       0.71      0.69      0.70       572\n",
      "           8       0.78      0.66      0.72       433\n",
      "           9       0.76      0.81      0.78       359\n",
      "\n",
      "    accuracy                           0.77      5683\n",
      "   macro avg       0.77      0.77      0.77      5683\n",
      "weighted avg       0.77      0.77      0.77      5683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "#获取朴素贝叶斯分类器\n",
    "classifier = MultinomialNB(alpha=1.0,  # 拉普拉斯平滑\n",
    "                          fit_prior=True,  #否要考虑先验概率\n",
    "                          class_prior=None)\n",
    "\n",
    "#进行训练                        \n",
    "classifier.fit(train_feature_list, train_class_list)\n",
    "# 在验证集上进行验证\n",
    "predict = classifier.predict(test_feature_list)\n",
    "test_accuracy = accuracy_score(predict,test_class_list)\n",
    "print(\"accuracy_score: %.4lf\"%(test_accuracy))\n",
    "print(\"Classification report for classifier:\\n\",classification_report(test_class_list, predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#加载句子，对句子进行预处理\n",
    "def load_sentence(sentence):\n",
    "    sentence = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\')]+|[+——()?【】“”！，。？、~@#￥%……&*（）《》：]+\", \"\",sentence) #去除标点符号\n",
    "    sentence = jieba.lcut(sentence, cut_all=False)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果: [['中国', '稳健', '前行', '应对', '风险', '挑战', '必须', '发挥', '制度', '优势']]\n",
      "财经\n"
     ]
    }
   ],
   "source": [
    "lab = [ '文化', '娱乐', '体育', '财经','房产', '汽车', '教育', '科技', '国际', '证券']\n",
    "\n",
    "p_data = '【中国稳健前行】应对风险挑战必须发挥制度优势'\n",
    "sentence = load_sentence(p_data)\n",
    "sentence= [sentence]\n",
    "print('分词结果:', sentence)\n",
    "#形成特征向量\n",
    "p_words = get_text_features(sentence,sentence,feature_words)\n",
    "res = classifier.predict(p_words[0])\n",
    "print(\"所属类型：\",lab[int(res)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

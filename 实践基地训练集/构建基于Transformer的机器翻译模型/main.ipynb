{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1.加载开发环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "print(paddle.__version__)\n",
    "# cpu/gpu环境选择，在 paddle.set_device() 输入对应运行设备。\n",
    "# device = paddle.set_device('gpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. 统计数据集信息，确定句子长度，我们采用包含90%句子长度的长度值作为句子的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 统计数据集中句子的长度等信息\r\n",
    "lines =  open('data/data78721/cmn.txt','r',encoding='utf-8').readlines()\r\n",
    "print(len(lines))\r\n",
    "datas = []\r\n",
    "dic_en = {}\r\n",
    "dic_cn = {}\r\n",
    "for line in lines:\r\n",
    "    ll = line.strip().split('\\t')\r\n",
    "    if len(ll)<2:\r\n",
    "        continue\r\n",
    "    datas.append([ll[0].lower().split(' ')[1:-1],list(ll[1])])\r\n",
    "    # print(ll[0])\r\n",
    "    if len(ll[0].split(' ')) not in dic_en:\r\n",
    "        dic_en[len(ll[0].split(' '))] = 1\r\n",
    "    else:\r\n",
    "        dic_en[len(ll[0].split(' '))] +=1\r\n",
    "    if len(ll[1]) not in dic_cn:\r\n",
    "        dic_cn[len(ll[1])] = 1\r\n",
    "    else:\r\n",
    "        dic_cn[len(ll[1])] +=1\r\n",
    "keys_en = list(dic_en.keys())\r\n",
    "keys_en.sort()\r\n",
    "count = 0\r\n",
    "# print('英文长度统计：')\r\n",
    "for k in keys_en:\r\n",
    "    count += dic_en[k]\r\n",
    "    # print(k,dic_en[k],count/len(lines))\r\n",
    "\r\n",
    "keys_cn = list(dic_cn.keys())\r\n",
    "keys_cn.sort()\r\n",
    "count = 0\r\n",
    "# print('中文长度统计：')\r\n",
    "for k in keys_cn:\r\n",
    "    count += dic_cn[k]\r\n",
    "    # print(k,dic_cn[k],count/len(lines))\r\n",
    " \r\n",
    "en_length = 10\r\n",
    "cn_length = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3. 构建中文词表、英文词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 构建中英文词表\r\n",
    "en_vocab = {}\r\n",
    "cn_vocab = {}\r\n",
    "\r\n",
    "en_vocab['<pad>'], en_vocab['<bos>'], en_vocab['<eos>'] = 0, 1, 2\r\n",
    "cn_vocab['<pad>'], cn_vocab['<bos>'], cn_vocab['<eos>'] = 0, 1, 2\r\n",
    "en_idx, cn_idx = 3, 3\r\n",
    "for en, cn in datas:\r\n",
    "    # print(en,cn)\r\n",
    "    for w in en:\r\n",
    "        if w not in en_vocab:\r\n",
    "            en_vocab[w] = en_idx\r\n",
    "            en_idx += 1\r\n",
    "    for w in cn:\r\n",
    "        if w not in cn_vocab:\r\n",
    "            cn_vocab[w] = cn_idx\r\n",
    "            cn_idx += 1\r\n",
    "\r\n",
    "print(len(list(en_vocab)))\r\n",
    "print(len(list(cn_vocab)))\r\n",
    "'''\r\n",
    "英文词表长度：6057\r\n",
    "中文词表长度：3533\r\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. 创建数据集\n",
    "接下来根据词表，我们将会创建一份实际的用于训练的用numpy array组织起来的数据集。\n",
    "- 所有的句子都通过补充成为了长度相同的句子。\n",
    "- 对于英文句子（源语言），我们将其反转了过来，这会带来更好的翻译的效果。 \n",
    "- 所创建的padded_cn_label_sents是训练过程中的预测的目标，即，每个中文的当前词去预测下一个词是什么词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "padded_en_sents = []\r\n",
    "padded_cn_sents = []\r\n",
    "padded_cn_label_sents = []\r\n",
    "for en, cn in datas:\r\n",
    "    if len(en)>en_length:\r\n",
    "        en = en[:en_length]\r\n",
    "    if len(cn)>cn_length:\r\n",
    "        cn = cn[:cn_length]\r\n",
    "    padded_en_sent = en + ['<eos>'] + ['<pad>'] * (en_length - len(en))\r\n",
    "    padded_en_sent.reverse()\r\n",
    "\r\n",
    "    padded_cn_sent = ['<bos>'] + cn + ['<eos>'] + ['<pad>'] * (cn_length - len(cn))\r\n",
    "    padded_cn_label_sent = cn + ['<eos>'] + ['<pad>'] * (cn_length - len(cn) + 1)\r\n",
    "    \r\n",
    "    padded_en_sents.append(np.array([en_vocab[w] for w in padded_en_sent]))\r\n",
    "    padded_cn_sents.append(np.array([cn_vocab[w] for w in padded_cn_sent]) )\r\n",
    "    padded_cn_label_sents.append(np.array([cn_vocab[w] for w in padded_cn_label_sent]))\r\n",
    "\r\n",
    "train_en_sents = np.array(padded_en_sents)\r\n",
    "train_cn_sents = np.array(padded_cn_sents)\r\n",
    "train_cn_label_sents = np.array(padded_cn_label_sents)\r\n",
    " \r\n",
    "print(train_en_sents.shape)\r\n",
    "print(train_cn_sents.shape)\r\n",
    "print(train_cn_label_sents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5.构建基于Transformer的机器翻译模型\n",
    "- 首先定义超参数，用于后续模型的设计与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128\r\n",
    "hidden_size = 512\r\n",
    "num_encoder_lstm_layers = 1\r\n",
    "en_vocab_size = len(list(en_vocab))\r\n",
    "cn_vocab_size = len(list(cn_vocab))\r\n",
    "epochs = 20\r\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- 使用TransformerEncoder定义Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encoder: simply learn representation of source sentence\r\n",
    "class Encoder(paddle.nn.Layer):\r\n",
    "    def __init__(self,en_vocab_size, embedding_size,num_layers=2,head_number=2,middle_units=512):\r\n",
    "        super(Encoder, self).__init__()\r\n",
    "        self.emb = paddle.nn.Embedding(en_vocab_size, embedding_size,)\r\n",
    "        \"\"\"\r\n",
    "        d_model (int) - 输入输出的维度。\r\n",
    "        nhead (int) - 多头注意力机制的Head数量。\r\n",
    "        dim_feedforward (int) - 前馈神经网络中隐藏层的大小。\r\n",
    "        \"\"\"\r\n",
    "        encoder_layer = paddle.nn.TransformerEncoderLayer(embedding_size, head_number, middle_units)\r\n",
    "        self.encoder = paddle.nn.TransformerEncoder(encoder_layer, num_layers) \r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.emb(x)\r\n",
    "        en_out = self.encoder(x)\r\n",
    "        return en_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- 使用TransformerDecoder定义Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(paddle.nn.Layer):\n",
    "    def __init__(self,cn_vocab_size, embedding_size,num_layers=2,head_number=2,middle_units=512):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = paddle.nn.Embedding(cn_vocab_size, embedding_size)\n",
    "        \n",
    "        decoder_layer = paddle.nn.TransformerDecoderLayer(embedding_size, head_number, middle_units)\n",
    "        self.decoder = paddle.nn.TransformerDecoder(decoder_layer, num_layers) \n",
    "   \n",
    "        # for computing output logits\n",
    "        self.outlinear =paddle.nn.Linear(embedding_size, cn_vocab_size)\n",
    "\n",
    "    def forward(self, x,  encoder_outputs):\n",
    "        x = self.emb(x)\n",
    "        # dec_input, enc_output,self_attn_mask,  cross_attn_mask\n",
    "        de_out = self.decoder(x, encoder_outputs)\n",
    "        output = self.outlinear(de_out)\n",
    "        output = paddle.squeeze(output)\n",
    "        return  output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(en_vocab_size, embedding_size)\r\n",
    "decoder = Decoder(cn_vocab_size, embedding_size)\r\n",
    "\r\n",
    "opt = paddle.optimizer.Adam(learning_rate=0.0001,\r\n",
    "                            parameters=encoder.parameters() + decoder.parameters())\r\n",
    "\r\n",
    "for epoch in range(epochs):\r\n",
    "    print(\"epoch:{}\".format(epoch))\r\n",
    "\r\n",
    "    # shuffle training data\r\n",
    "    perm = np.random.permutation(len(train_en_sents))\r\n",
    "    train_en_sents_shuffled = train_en_sents[perm]\r\n",
    "    train_cn_sents_shuffled = train_cn_sents[perm]\r\n",
    "    train_cn_label_sents_shuffled = train_cn_label_sents[perm]\r\n",
    "    # print(train_en_sents_shuffled.shape[0],train_en_sents_shuffled.shape[1])\r\n",
    "    for iteration in range(train_en_sents_shuffled.shape[0] // batch_size):\r\n",
    "        x_data = train_en_sents_shuffled[(batch_size*iteration):(batch_size*(iteration+1))]\r\n",
    "        sent = paddle.to_tensor(x_data)\r\n",
    "        en_repr = encoder(sent)\r\n",
    "\r\n",
    "        x_cn_data = train_cn_sents_shuffled[(batch_size*iteration):(batch_size*(iteration+1))]\r\n",
    "        x_cn_label_data = train_cn_label_sents_shuffled[(batch_size*iteration):(batch_size*(iteration+1))]\r\n",
    " \r\n",
    "        loss = paddle.zeros([1]) \r\n",
    "        for i in range( cn_length + 2):\r\n",
    "            cn_word = paddle.to_tensor(x_cn_data[:,i:i+1])\r\n",
    "            cn_word_label = paddle.to_tensor(x_cn_label_data[:,i])\r\n",
    "\r\n",
    "            logits = decoder(cn_word, en_repr)\r\n",
    "            step_loss = F.cross_entropy(logits, cn_word_label)\r\n",
    "            loss += step_loss\r\n",
    "\r\n",
    "        loss = loss / (cn_length + 2)\r\n",
    "        if(iteration % 50 == 0):\r\n",
    "            print(\"iter {}, loss:{}\".format(iteration, loss.numpy()))\r\n",
    "\r\n",
    "        loss.backward()\r\n",
    "        opt.step()\r\n",
    "        opt.clear_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6. 使用上述训练好的模型进行测试\n",
    "- 随机从训练集中抽取几句话来进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder.eval()\r\n",
    "decoder.eval()\r\n",
    "\r\n",
    "num_of_exampels_to_evaluate = 10\r\n",
    "\r\n",
    "indices = np.random.choice(len(train_en_sents),  num_of_exampels_to_evaluate, replace=False)\r\n",
    "x_data = train_en_sents[indices]\r\n",
    "sent = paddle.to_tensor(x_data)\r\n",
    "en_repr = encoder(sent)\r\n",
    "\r\n",
    "word = np.array(\r\n",
    "    [[cn_vocab['<bos>']]] * num_of_exampels_to_evaluate\r\n",
    ")\r\n",
    "word = paddle.to_tensor(word)\r\n",
    " \r\n",
    "\r\n",
    "decoded_sent = []\r\n",
    "for i in range(cn_length + 2):\r\n",
    "    logits  = decoder(word, en_repr)\r\n",
    "    word = paddle.argmax(logits, axis=1)\r\n",
    "    decoded_sent.append(word.numpy())\r\n",
    "    word = paddle.unsqueeze(word, axis=-1)\r\n",
    "\r\n",
    "results = np.stack(decoded_sent, axis=1)\r\n",
    "for i in range(num_of_exampels_to_evaluate):\r\n",
    "    print('---------------------')\r\n",
    "    en_input = \" \".join(datas[indices[i]][0])\r\n",
    "    ground_truth_translate = \"\".join(datas[indices[i]][1])\r\n",
    "    model_translate = \"\"\r\n",
    "    for k in results[i]:\r\n",
    "        w = list(cn_vocab)[k]\r\n",
    "        if w != '<pad>' and w != '<eos>':\r\n",
    "            model_translate += w\r\n",
    "    print(en_input)\r\n",
    "    print(\"true: {}\".format(ground_truth_translate))\r\n",
    "    print(\"pred: {}\".format(model_translate))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

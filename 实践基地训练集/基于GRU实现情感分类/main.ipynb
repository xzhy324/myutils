{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 任务描述：\n",
    "\n",
    "本示例教程演示如何在IMDB数据集上用GRU网络完成文本分类的任务。\n",
    "\n",
    "IMDB数据集是一个对电影评论标注为正向评论与负向评论的数据集，共有25000条文本数据作为训练集，25000条文本数据作为测试集。\n",
    "该数据集的官方地址为： http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、环境设置\n",
    "\n",
    "本示例基于飞桨开源框架2.0版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n",
      "2021-06-03 19:45:46,492 - INFO - font search path ['/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/afm', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts']\n",
      "2021-06-03 19:45:46,998 - INFO - generated new fontManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import paddle.nn as nn\n",
    "\n",
    "print(paddle.__version__)  # 查看当前版本\n",
    "\n",
    "# cpu/gpu环境选择，在 paddle.set_device() 输入对应运行设备。\n",
    "device = paddle.set_device('gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、数据准备\n",
    "由于IMDB是NLP领域中常见的数据集，飞桨框架将其内置，路径为 `paddle.text.datasets.Imdb`。通过 `mode` 参数可以控制训练集与测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cache file /home/aistudio/.cache/paddle/dataset/imdb/imdb%2FaclImdb_v1.tar.gz not found, downloading https://dataset.bj.bcebos.com/imdb%2FaclImdb_v1.tar.gz \n",
      "Begin to download\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Download finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading finished\n"
     ]
    }
   ],
   "source": [
    "print('loading dataset...')\n",
    "train_dataset = paddle.text.datasets.Imdb(mode='train')\n",
    "test_dataset = paddle.text.datasets.Imdb(mode='test')\n",
    "print('loading finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "构建了训练集与测试集后，可以通过 `word_idx` 获取数据集的词表。在飞桨框架2.0版本中，推荐使用padding的方式来对同一个batch中长度不一的数据进行补齐，所以在字典中，我们还会添加一个特殊的<pad>词，用来在后续对batch中较短的句子进行填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the:0\n",
      "and:1\n",
      "a:2\n",
      "of:3\n",
      "to:4\n",
      "...\n",
      "virtual:5143\n",
      "warriors:5144\n",
      "widely:5145\n",
      "<unk>:5146\n",
      "<pad>:5147\n",
      "totally 5148 words\n"
     ]
    }
   ],
   "source": [
    "word_dict = train_dataset.word_idx  # 获取数据集的词表\n",
    "\n",
    "# add a pad token to the dict for later padding the sequence\n",
    "word_dict['<pad>'] = len(word_dict)\n",
    "\n",
    "for k in list(word_dict)[:5]:\n",
    "    print(\"{}:{}\".format(k.decode('ASCII'), word_dict[k]))\n",
    "\n",
    "print(\"...\")\n",
    "\n",
    "for k in list(word_dict)[-5:]:\n",
    "    print(\"{}:{}\".format(k if isinstance(k, str) else k.decode('ASCII'), word_dict[k]))\n",
    "\n",
    "print(\"totally {} words\".format(len(word_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1 参数设置\n",
    "\n",
    "在这里我们设置一下词表大小，`embedding`的大小，batch_size，等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5149\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_dict) + 1\n",
    "print(vocab_size)\n",
    "emb_size = 256\n",
    "seq_len = 200\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "pad_id = word_dict['<pad>']\n",
    "\n",
    "classes = ['negative', 'positive']\n",
    "\n",
    "# 生成句子列表\n",
    "def ids_to_str(ids):\n",
    "    # print(ids)\n",
    "    words = []\n",
    "    for k in ids:\n",
    "        w = list(word_dict)[k]\n",
    "        words.append(w if isinstance(w, str) else w.decode('ASCII'))\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在这里，取出一条数据打印出来看看，可以用 `docs` 获取数据的list，用 `labels` 获取数据的label值，打印出来对数据有一个初步的印象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence list id is: [5146, 43, 71, 6, 1092, 14, 0, 878, 130, 151, 5146, 18, 281, 747, 0, 5146, 3, 5146, 2165, 37, 5146, 46, 5, 71, 4089, 377, 162, 46, 5, 32, 1287, 300, 35, 203, 2136, 565, 14, 2, 253, 26, 146, 61, 372, 1, 615, 5146, 5, 30, 0, 50, 3290, 6, 2148, 14, 0, 5146, 11, 17, 451, 24, 4, 127, 10, 0, 878, 130, 43, 2, 50, 5146, 751, 5146, 5, 2, 221, 3727, 6, 9, 1167, 373, 9, 5, 5146, 7, 5, 1343, 13, 2, 5146, 1, 250, 7, 98, 4270, 56, 2316, 0, 928, 11, 11, 9, 16, 5, 5146, 5146, 6, 50, 69, 27, 280, 27, 108, 1045, 0, 2633, 4177, 3180, 17, 1675, 1, 2571]\n",
      "sentence label id is: 0\n",
      "--------------------------\n",
      "sentence list is:  <unk> has much in common with the third man another <unk> film set among the <unk> of <unk> europe like <unk> there is much inventive camera work there is an innocent american who gets emotionally involved with a woman he doesnt really understand and whose <unk> is all the more striking in contrast with the <unk> br but id have to say that the third man has a more <unk> storyline <unk> is a bit disjointed in this respect perhaps this is <unk> it is presented as a <unk> and making it too coherent would spoil the effect br br this movie is <unk> <unk> in more than one sense one never sees the sun shine grim but intriguing and frightening\n",
      "sentence label is:  negative\n"
     ]
    }
   ],
   "source": [
    "# 取出来第一条数据看看样子。\n",
    "sent = train_dataset.docs[0]\n",
    "label = train_dataset.labels[1]\n",
    "print('sentence list id is:', sent)\n",
    "print('sentence label id is:', label)\n",
    "print('--------------------------')\n",
    "print('sentence list is: ', ids_to_str(sent))\n",
    "print('sentence label is: ', classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 用padding的方式对齐数据\n",
    "\n",
    "文本数据中，每一句话的长度都是不一样的，为了方便后续的神经网络的计算，常见的处理方式是把数据集中的数据都统一成同样长度的数据。这包括：对于较长的数据进行截断处理，对于较短的数据用特殊的词`<pad>`进行填充。接下来的代码会对数据集中的数据进行这样的处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 200)\n",
      "(25000, 1)\n",
      "(25000, 200)\n",
      "(25000, 1)\n",
      "<unk> has much in common with the third man another <unk> film set among the <unk> of <unk> europe like <unk> there is much inventive camera work there is an innocent american who gets emotionally involved with a woman he doesnt really understand and whose <unk> is all the more striking in contrast with the <unk> br but id have to say that the third man has a more <unk> storyline <unk> is a bit disjointed in this respect perhaps this is <unk> it is presented as a <unk> and making it too coherent would spoil the effect br br this movie is <unk> <unk> in more than one sense one never sees the sun shine grim but intriguing and frightening <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<unk> is the most original movie ive seen in years if you like unique thrillers that are influenced by film noir then this is just the right cure for all of those hollywood summer <unk> <unk> the theaters these days von <unk> <unk> like breaking the waves have gotten more <unk> but this is really his best work it is <unk> without being distracting and offers the perfect combination of suspense and dark humor its too bad he decided <unk> cameras were the wave of the future its hard to say who talked him away from the style he <unk> here but its everyones loss that he went into his heavily <unk> <unk> direction instead <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<unk> von <unk> is never <unk> in trying out new techniques some of them are very original while others are best <unk> br he depicts <unk> germany as a <unk> train journey with so many cities lying in ruins <unk> <unk> a young american of german descent feels <unk> to help in their <unk> it is not a simple task as he quickly finds outbr br his uncle finds him a job as a night <unk> on the <unk> <unk> line his job is to <unk> to the needs of the passengers when the shoes are <unk> a <unk> mark is made on the <unk> a terrible argument <unk> when a passengers shoes are not <unk> despite the fact they have been <unk> there are many <unk> to the german <unk> of <unk> to such stupid <unk> br the <unk> journey is like an <unk> <unk> mans <unk> through life with all its <unk> and <unk> in one sequence <unk> <unk> through the back <unk> to discover them filled with <unk> bodies appearing to have just escaped from <unk> these images horrible as they are are <unk> as in a dream each with its own terrible impact yet <unk> br\n"
     ]
    }
   ],
   "source": [
    "# 读取数据归一化处理\n",
    "def create_padded_dataset(dataset):\n",
    "    padded_sents = []\n",
    "    labels = []\n",
    "    for batch_id, data in enumerate(dataset):\n",
    "        sent, label = data[0], data[1]\n",
    "        padded_sent = np.concatenate([sent[:seq_len], [pad_id] * (seq_len - len(sent))]).astype('int32')\n",
    "        padded_sents.append(padded_sent)\n",
    "        labels.append(label)\n",
    "    return np.array(padded_sents), np.array(labels)\n",
    "\n",
    "# 对train、test数据进行实例化\n",
    "train_sents, train_labels = create_padded_dataset(train_dataset)\n",
    "test_sents, test_labels = create_padded_dataset(test_dataset)\n",
    "\n",
    "# 查看数据大小及举例内容\n",
    "print(train_sents.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_sents.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "for sent in train_sents[:3]:\n",
    "    print(ids_to_str(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 用Dataset 与 DataLoader 加载\n",
    "将前面准备好的训练集与测试集用Dataset 与 DataLoader封装后，完成数据的加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(paddle.io.Dataset):\n",
    "    '''\n",
    "    继承paddle.io.Dataset类进行封装数据\n",
    "    '''\n",
    "    def __init__(self, sents, labels):\n",
    "        self.sents = sents\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.sents[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "    \n",
    "train_dataset = IMDBDataset(train_sents, train_labels)\n",
    "test_dataset = IMDBDataset(test_sents, test_labels)\n",
    "\n",
    "train_loader = paddle.io.DataLoader(train_dataset, return_list=True,\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = paddle.io.DataLoader(test_dataset, return_list=True,\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、模型配置\n",
    "\n",
    "本示例中，我们将会使用一个不考虑词的顺序的GRU的网络，在查找到每个词对应的embedding后，简单的取平均，作为一个句子的表示。然后用`Linear`进行线性变换。为了防止过拟合，我们还使用了`Dropout`。\n",
    "\n",
    " \n",
    "GRU是LSTM网络的一种效果很好的变体，它较LSTM网络的结构更加简单，而且效果也很。    \n",
    "因此也是当前非常流形的一种网络。GRU既然是LSTM的变体，因此也是可以解决RNN网络中的长依赖问题。   \n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/e73077399b2649289ab9a4faa4fb76bdc3728219ba1242df8f90aa9a87d3e024)  \n",
    "图中的zt和rt分别表示更新门和重置门。  \n",
    "更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。  \n",
    "重置门控制前一状态有多少信息被写入到当前的候选集 h~t 上，重置门越小，前一状态的信息被写入的越少。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn\r\n",
    "import paddle\r\n",
    "\r\n",
    "# 定义GRU网络\r\n",
    "class MyGRU(paddle.nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(MyGRU, self).__init__()\r\n",
    "        self.embedding = nn.Embedding(vocab_size, 256)\r\n",
    "        self.gru = nn.GRU(256, 256, num_layers=2, direction='bidirectional',dropout=0.5)\r\n",
    "        self.linear = nn.Linear(in_features=256*2, out_features=2)\r\n",
    "        self.dropout = nn.Dropout(0.5)\r\n",
    "    \r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        emb = self.dropout(self.embedding(inputs))\r\n",
    "        #output形状大小为[batch_size,seq_len,num_directions * hidden_size]\r\n",
    "        #hidden形状大小为[num_layers * num_directions, batch_size, hidden_size]\r\n",
    "        #把前向的hidden与后向的hidden合并在一起\r\n",
    "        output, hidden = self.gru(emb)\r\n",
    "        hidden = paddle.concat((hidden[-2,:,:], hidden[-1,:,:]), axis = 1)\r\n",
    "        #hidden形状大小为[batch_size, hidden_size * num_directions]\r\n",
    "        hidden = self.dropout(hidden)\r\n",
    "        return self.linear(hidden) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 可视化定义\n",
    "def draw_process(title,color,iters,data,label):\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"iter\", fontsize=20)\n",
    "    plt.ylabel(label, fontsize=20)\n",
    "    plt.plot(iters, data,color=color,label=label) \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 0, loss is: [0.69404197]\n",
      "epoch: 0, batch_id: 500, loss is: [0.7091604]\n",
      "[validation] accuracy: 0.8016164898872375, loss: 0.41171079874038696\n",
      "epoch: 1, batch_id: 0, loss is: [0.41999102]\n",
      "epoch: 1, batch_id: 500, loss is: [0.29760724]\n",
      "[validation] accuracy: 0.8513924479484558, loss: 0.3458637297153473\n",
      "epoch: 2, batch_id: 0, loss is: [0.32252336]\n",
      "epoch: 2, batch_id: 500, loss is: [0.23978208]\n",
      "[validation] accuracy: 0.8573143482208252, loss: 0.3317616581916809\n",
      "epoch: 3, batch_id: 0, loss is: [0.08354414]\n",
      "epoch: 3, batch_id: 500, loss is: [0.15088022]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-511a33a949a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-511a33a949a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-32614bef61f4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#hidden形状大小为[num_layers * num_directions, batch_size, hidden_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#把前向的hidden与后向的hidden合并在一起\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#hidden形状大小为[batch_size, hidden_size * num_directions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, initial_states, sequence_length)\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcould_use_cudnn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# Add CPU kernel and dispatch in backend later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cudnn_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         states = split_states(initial_states, self.num_directions == 2,\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/rnn.py\u001b[0m in \u001b[0;36m_cudnn_impl\u001b[0;34m(self, inputs, initial_states, sequence_length)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         self._helper.append_op(\n\u001b[0;32m-> 1024\u001b[0;31m             type=\"rnn\", inputs=inputs, outputs=outputs, attrs=attrs)\n\u001b[0m\u001b[1;32m   1025\u001b[0m         out = paddle.tensor.transpose(out,\n\u001b[1;32m   1026\u001b[0m                                       [1, 0, 2]) if not self.time_major else out\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layer_object_helper.py\u001b[0m in \u001b[0;36mappend_op\u001b[0;34m(self, type, inputs, outputs, attrs, stop_gradient)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             stop_gradient=stop_gradient)\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_multiple_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36mappend_op\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3016\u001b[0m                                        \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3017\u001b[0m                                        \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3018\u001b[0;31m                                        kwargs.get(\"stop_gradient\", False))\n\u001b[0m\u001b[1;32m   3019\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m             \u001b[0mop_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/tracer.py\u001b[0m in \u001b[0;36mtrace_op\u001b[0;34m(self, type, inputs, outputs, attrs, stop_gradient)\u001b[0m\n\u001b[1;32m     43\u001b[0m         self.trace(type, inputs, outputs, attrs,\n\u001b[1;32m     44\u001b[0m                    \u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_expected_place\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_grad\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                    not stop_gradient)\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 对模型进行封装\n",
    "def train(model):\n",
    "    model.train()\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\n",
    "    steps = 0\n",
    "    Iters, total_loss, total_acc = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            steps += 1\n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "            \n",
    "            logits = model(sent)\n",
    "            loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "\n",
    "            if batch_id % 500 == 0:  # 500个epoch输出一次结果\n",
    "                Iters.append(steps)\n",
    "                total_loss.append(loss.numpy()[0])\n",
    "                total_acc.append(acc.numpy()[0])\n",
    "\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, loss.numpy()))\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        # evaluate model after one epoch\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        \n",
    "        for batch_id, data in enumerate(test_loader):\n",
    "            \n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "\n",
    "            logits = model(sent)\n",
    "            loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "            \n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "        \n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "\n",
    "        print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # 保存模型\n",
    "        paddle.save(model.state_dict(),str(epoch)+\"_model_final.pdparams\")\n",
    "    \n",
    "    # 可视化查看\n",
    "    draw_process(\"trainning loss\",\"red\",Iters,total_loss,\"trainning loss\")\n",
    "    draw_process(\"trainning acc\",\"green\",Iters,total_acc,\"trainning acc\")\n",
    "        \n",
    "model = MyGRU()\n",
    "train(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 五、模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "模型评估\n",
    "'''\n",
    "model_state_dict = paddle.load('6_model_final.pdparams')  # 导入模型\n",
    "model = MyGRU()\n",
    "model.set_state_dict(model_state_dict) \n",
    "model.eval()\n",
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "for batch_id, data in enumerate(test_loader):\n",
    "    \n",
    "    sent = data[0]\n",
    "    label = data[1]\n",
    "\n",
    "    logits = model(sent)\n",
    "    loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "    acc = paddle.metric.accuracy(logits, label)\n",
    "    \n",
    "    accuracies.append(acc.numpy())\n",
    "    losses.append(loss.numpy())\n",
    "\n",
    "avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 六、模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ids_to_str(ids):\n",
    "    words = []\n",
    "    for k in ids:\n",
    "        w = list(word_dict)[k]\n",
    "        words.append(w if isinstance(w, str) else w.decode('ASCII'))\n",
    "    return \" \".join(words)\n",
    "\n",
    "label_map = {0:\"negative\", 1:\"positive\"}\n",
    "\n",
    "# 导入模型\n",
    "model_state_dict = paddle.load('6_model_final.pdparams')\n",
    "model = MyGRU()\n",
    "model.set_state_dict(model_state_dict) \n",
    "model.eval()\n",
    "\n",
    "for batch_id, data in enumerate(test_loader):\n",
    "    \n",
    "    sent = data[0]\n",
    "    results = model(sent)\n",
    "\n",
    "    predictions = []\n",
    "    for probs in results:\n",
    "        # 映射分类label\n",
    "        idx = np.argmax(probs)\n",
    "        labels = label_map[idx]\n",
    "        predictions.append(labels)\n",
    "    \n",
    "    for i,pre in enumerate(predictions):\n",
    "        print(' 数据: {} \\n 情感: {}'.format(ids_to_str(sent[0]), pre))\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

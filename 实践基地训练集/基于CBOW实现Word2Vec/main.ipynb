{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# è¯å‘é‡è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œè¯å‘é‡æ˜¯è¡¨ç¤ºè‡ªç„¶è¯­è¨€é‡Œå•è¯çš„ä¸€ç§æ–¹æ³•ï¼Œå³æŠŠæ¯ä¸ªè¯éƒ½è¡¨ç¤ºä¸ºä¸€ä¸ªNç»´ç©ºé—´å†…çš„ç‚¹ï¼Œå³ä¸€ä¸ªé«˜ç»´ç©ºé—´å†…çš„å‘é‡ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œå®ç°æŠŠè‡ªç„¶è¯­è¨€è®¡ç®—è½¬æ¢ä¸ºå‘é‡è®¡ç®—ã€‚\n",
    "\n",
    "å¦‚ **å›¾1** æ‰€ç¤ºçš„è¯å‘é‡è®¡ç®—ä»»åŠ¡ä¸­ï¼Œå…ˆæŠŠæ¯ä¸ªè¯ï¼ˆå¦‚queenï¼Œkingç­‰ï¼‰è½¬æ¢æˆä¸€ä¸ªé«˜ç»´ç©ºé—´çš„å‘é‡ï¼Œè¿™äº›å‘é‡åœ¨ä¸€å®šæ„ä¹‰ä¸Šå¯ä»¥ä»£è¡¨è¿™ä¸ªè¯çš„è¯­ä¹‰ä¿¡æ¯ã€‚å†é€šè¿‡è®¡ç®—è¿™äº›å‘é‡ä¹‹é—´çš„è·ç¦»ï¼Œå°±å¯ä»¥è®¡ç®—å‡ºè¯è¯­ä¹‹é—´çš„å…³è”å…³ç³»ï¼Œä»è€Œè¾¾åˆ°è®©è®¡ç®—æœºåƒè®¡ç®—æ•°å€¼ä¸€æ ·å»è®¡ç®—è‡ªç„¶è¯­è¨€çš„ç›®çš„ã€‚\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/401409b5b36f4c55ade6078ba198634ee54b8f27e51d4729b66b32e844689969\" width=\"1000\" ></center>\n",
    "<center>å›¾1ï¼šè¯å‘é‡è®¡ç®—ç¤ºæ„å›¾</center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "word2vecåŒ…å«ä¸¤ä¸ªç»å…¸æ¨¡å‹ï¼ŒCBOWï¼ˆContinuous Bag-of-Wordsï¼‰å’ŒSkip-gramï¼Œå¦‚ **å›¾2** æ‰€ç¤ºã€‚\n",
    "\n",
    "- **CBOW**ï¼šé€šè¿‡ä¸Šä¸‹æ–‡çš„è¯å‘é‡æ¨ç†ä¸­å¿ƒè¯ã€‚\n",
    "- **Skip-gram**ï¼šæ ¹æ®ä¸­å¿ƒè¯æ¨ç†ä¸Šä¸‹æ–‡ã€‚\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/5d0fc1ad327e438e9101d059772be6732ee808bd80e04e1eabc575cd28e61a69\" width=\"1000\" ></center>\n",
    "<center><br>å›¾2ï¼šCBOWå’ŒSkip-gramè¯­ä¹‰å­¦ä¹ ç¤ºæ„å›¾</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## CBOWçš„ç®—æ³•å®ç°\n",
    "\n",
    "æˆ‘ä»¬ä»¥è¿™å¥è¯ï¼šâ€œPineapples are spiked and yellowâ€ä¸ºä¾‹ä»‹ç»CBOWç®—æ³•å®ç°ã€‚\n",
    "\n",
    "å¦‚ **å›¾3** æ‰€ç¤ºï¼ŒCBOWæ˜¯ä¸€ä¸ªå…·æœ‰3å±‚ç»“æ„çš„ç¥ç»ç½‘ç»œï¼Œåˆ†åˆ«æ˜¯ï¼š\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/72397490c0ba499692cff31484431c57bc9d20f7ef344454868e12d628ec5bd3\" width=\"400\" ></center>\n",
    "<center><br>å›¾3ï¼šCBOWçš„ç®—æ³•å®ç°</br></center>\n",
    "<br></br>\n",
    "\n",
    "* **è¾“å…¥å±‚ï¼š** ä¸€ä¸ªå½¢çŠ¶ä¸ºCÃ—Vçš„one-hotå¼ é‡ï¼Œå…¶ä¸­Cä»£è¡¨ä¸Šçº¿æ–‡ä¸­è¯çš„ä¸ªæ•°ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªå¶æ•°ï¼Œæˆ‘ä»¬å‡è®¾ä¸º4ï¼›Vè¡¨ç¤ºè¯è¡¨å¤§å°ï¼Œæˆ‘ä»¬å‡è®¾ä¸º5000ï¼Œè¯¥å¼ é‡çš„æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡è¯çš„one-hotå‘é‡è¡¨ç¤ºï¼Œæ¯”å¦‚â€œPineapples, are, and, yellowâ€ã€‚\n",
    "* **éšè—å±‚ï¼š** ä¸€ä¸ªå½¢çŠ¶ä¸ºVÃ—Nçš„å‚æ•°å¼ é‡W1ï¼Œä¸€èˆ¬ç§°ä¸ºword-embeddingï¼ŒNè¡¨ç¤ºæ¯ä¸ªè¯çš„è¯å‘é‡é•¿åº¦ï¼Œæˆ‘ä»¬å‡è®¾ä¸º128ã€‚è¾“å…¥å¼ é‡å’Œword embedding W1è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œå°±ä¼šå¾—åˆ°ä¸€ä¸ªå½¢çŠ¶ä¸ºCÃ—Nçš„å¼ é‡ã€‚ç»¼åˆè€ƒè™‘ä¸Šä¸‹æ–‡ä¸­æ‰€æœ‰è¯çš„ä¿¡æ¯å»æ¨ç†ä¸­å¿ƒè¯ï¼Œå› æ­¤å°†ä¸Šä¸‹æ–‡ä¸­Cä¸ªè¯ç›¸åŠ å¾—ä¸€ä¸ª1Ã—Nçš„å‘é‡ï¼Œæ˜¯æ•´ä¸ªä¸Šä¸‹æ–‡çš„ä¸€ä¸ªéšå«è¡¨ç¤ºã€‚\n",
    "* **è¾“å‡ºå±‚ï¼š** åˆ›å»ºå¦ä¸€ä¸ªå½¢çŠ¶ä¸ºNÃ—Vçš„å‚æ•°å¼ é‡ï¼Œå°†éšè—å±‚å¾—åˆ°çš„1Ã—Nçš„å‘é‡ä¹˜ä»¥è¯¥NÃ—Vçš„å‚æ•°å¼ é‡ï¼Œå¾—åˆ°äº†ä¸€ä¸ªå½¢çŠ¶ä¸º1Ã—Vçš„å‘é‡ã€‚æœ€ç»ˆï¼Œ1Ã—Vçš„å‘é‡ä»£è¡¨äº†ä½¿ç”¨ä¸Šä¸‹æ–‡å»æ¨ç†ä¸­å¿ƒè¯ï¼Œæ¯ä¸ªå€™é€‰è¯çš„æ‰“åˆ†ï¼Œå†ç»è¿‡softmaxå‡½æ•°çš„å½’ä¸€åŒ–ï¼Œå³å¾—åˆ°äº†å¯¹ä¸­å¿ƒè¯çš„æ¨ç†æ¦‚ç‡ï¼š\n",
    "\n",
    "$$ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥({O_i})= \\frac{exp({O_i})}{\\sum_jexp({O_j})}$$\n",
    "\n",
    "åœ¨å®é™…æ“ä½œä¸­ï¼Œä½¿ç”¨ä¸€ä¸ªæ»‘åŠ¨çª—å£ï¼ˆä¸€èˆ¬æƒ…å†µä¸‹ï¼Œé•¿åº¦æ˜¯å¥‡æ•°ï¼‰ï¼Œä»å·¦åˆ°å³å¼€å§‹æ‰«æå½“å‰å¥å­ã€‚æ¯ä¸ªæ‰«æå‡ºæ¥çš„ç‰‡æ®µè¢«å½“æˆä¸€ä¸ªå°å¥å­ï¼Œæ¯ä¸ªå°å¥å­ä¸­é—´çš„è¯è¢«è®¤ä¸ºæ˜¯ä¸­å¿ƒè¯ï¼Œå…¶ä½™çš„è¯è¢«è®¤ä¸ºæ˜¯è¿™ä¸ªä¸­å¿ƒè¯çš„ä¸Šä¸‹æ–‡ã€‚\n",
    "\n",
    "### CBOWçš„å®é™…å®ç°\n",
    "\n",
    "åœ¨å®é™…ä¸­ï¼Œä¸ºé¿å…è¿‡äºåºå¤§çš„è®¡ç®—é‡ï¼Œæˆ‘ä»¬é€šå¸¸é‡‡ç”¨è´Ÿé‡‡æ ·çš„æ–¹æ³•ï¼Œæ¥é¿å…æŸ¥è¯¢æ•´ä¸ªæ­¤è¡¨ï¼Œä»è€Œå°†å¤šåˆ†ç±»é—®é¢˜è½¬æ¢ä¸ºäºŒåˆ†ç±»é—®é¢˜ã€‚å…·ä½“å®ç°è¿‡ç¨‹**å¦‚å›¾6**ï¼š\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/517260aee7f24b97bb8e42774daf1b4ee5aaa5824d004ed1854a04bde4a1c8ac)\n",
    "<center><br>å›¾6ï¼šCBOWç®—æ³•çš„å®é™…å®ç°</br></center>\n",
    "<br></br>\n",
    "\n",
    "åœ¨å®ç°çš„è¿‡ç¨‹ä¸­ï¼Œé€šå¸¸ä¼šè®©æ¨¡å‹æ¥æ”¶3ä¸ªtensorè¾“å…¥ï¼š\n",
    "\n",
    "- ä»£è¡¨ä¸Šä¸‹æ–‡å•è¯çš„tensorï¼šå‡è®¾æˆ‘ä»¬ç§°ä¹‹ä¸ºcontext_words $V$ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¸ªtensoræ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º[batch_size, vocab_size]çš„one-hot tensorï¼Œè¡¨ç¤ºåœ¨ä¸€ä¸ªmini-batchä¸­æ¯ä¸ªä¸­å¿ƒè¯å…·ä½“çš„IDã€‚\n",
    "\n",
    "- ä»£è¡¨ç›®æ ‡è¯çš„tensorï¼šå‡è®¾æˆ‘ä»¬ç§°ä¹‹ä¸ºtarget_words $T$ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¸ªtensoråŒæ ·æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º[batch_size, vocab_size]çš„one-hot tensorï¼Œè¡¨ç¤ºåœ¨ä¸€ä¸ªmini-batchä¸­æ¯ä¸ªç›®æ ‡è¯å…·ä½“çš„IDã€‚\n",
    "\n",
    "- ä»£è¡¨ç›®æ ‡è¯æ ‡ç­¾çš„tensorï¼šå‡è®¾æˆ‘ä»¬ç§°ä¹‹ä¸ºlabels $L$ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¸ªtensoræ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º[batch_size, 1]çš„tensorï¼Œæ¯ä¸ªå…ƒç´ ä¸æ˜¯0å°±æ˜¯1ï¼ˆ0ï¼šè´Ÿæ ·æœ¬ï¼Œ1ï¼šæ­£æ ·æœ¬ï¼‰ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ä½¿ç”¨é£æ¡¨å®ç° CBOW\n",
    "\n",
    "æ¥ä¸‹æ¥æˆ‘ä»¬å°†å­¦ä¹ ä½¿ç”¨é£æ¡¨å®ç°CBOWæ¨¡å‹çš„æ–¹æ³•ã€‚åœ¨é£æ¡¨ä¸­ï¼Œä¸åŒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹åŸºæœ¬ä¸€è‡´ï¼Œæµç¨‹å¦‚ä¸‹ï¼š\n",
    "\n",
    "1. **æ•°æ®å¤„ç†**ï¼šé€‰æ‹©éœ€è¦ä½¿ç”¨çš„æ•°æ®ï¼Œå¹¶åšå¥½å¿…è¦çš„é¢„å¤„ç†å·¥ä½œã€‚\n",
    "\n",
    "2. **ç½‘ç»œå®šä¹‰**ï¼šä½¿ç”¨é£æ¡¨å®šä¹‰å¥½ç½‘ç»œç»“æ„ï¼ŒåŒ…æ‹¬è¾“å…¥å±‚ï¼Œä¸­é—´å±‚ï¼Œè¾“å‡ºå±‚ï¼ŒæŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ã€‚\n",
    "\n",
    "3. **ç½‘ç»œè®­ç»ƒ**ï¼šå°†å‡†å¤‡å¥½çš„æ•°æ®é€å…¥ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ï¼Œå¹¶è§‚å¯Ÿå­¦ä¹ çš„è¿‡ç¨‹æ˜¯å¦æ­£å¸¸ï¼Œå¦‚æŸå¤±å‡½æ•°å€¼æ˜¯å¦åœ¨é™ä½ï¼Œä¹Ÿå¯ä»¥æ‰“å°ä¸€äº›ä¸­é—´æ­¥éª¤çš„ç»“æœå‡ºæ¥ç­‰ã€‚\n",
    "\n",
    "4. **ç½‘ç»œè¯„ä¼°**ï¼šä½¿ç”¨æµ‹è¯•é›†åˆæµ‹è¯•è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œï¼Œçœ‹çœ‹è®­ç»ƒæ•ˆæœå¦‚ä½•ã€‚\n",
    "\n",
    "åœ¨æ•°æ®å¤„ç†å‰ï¼Œéœ€è¦å…ˆåŠ è½½é£æ¡¨å¹³å°ï¼ˆå¦‚æœç”¨æˆ·åœ¨æœ¬åœ°ä½¿ç”¨ï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…é£æ¡¨ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from collections import OrderedDict \n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### æ•°æ®å¤„ç†\n",
    "\n",
    "é¦–å…ˆï¼Œæ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„è¯­æ–™ç”¨äºè®­ç»ƒword2vecæ¨¡å‹ã€‚æˆ‘ä»¬é€‰æ‹©text8æ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†é‡ŒåŒ…å«äº†å¤§é‡ä»ç»´åŸºç™¾ç§‘æ”¶é›†åˆ°çš„è‹±æ–‡è¯­æ–™ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç ä¸‹è½½æ•°æ®é›†ï¼Œä¸‹è½½åçš„æ–‡ä»¶è¢«ä¿å­˜åœ¨å½“å‰ç›®å½•çš„text8.txtæ–‡ä»¶å†…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ä¸‹è½½è¯­æ–™ç”¨æ¥è®­ç»ƒword2vec\n",
    "def download():\n",
    "    #å¯ä»¥ä»ç™¾åº¦äº‘æœåŠ¡å™¨ä¸‹è½½ä¸€äº›å¼€æºæ•°æ®é›†ï¼ˆdataset.bj.bcebos.comï¼‰\n",
    "    corpus_url = \"https://dataset.bj.bcebos.com/word2vec/text8.txt\"\n",
    "    #ä½¿ç”¨pythonçš„requestsåŒ…ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°\n",
    "    web_request = requests.get(corpus_url)\n",
    "    corpus = web_request.content\n",
    "    #æŠŠä¸‹è½½åçš„æ–‡ä»¶å­˜å‚¨åœ¨å½“å‰ç›®å½•çš„text8.txtæ–‡ä»¶å†…\n",
    "    with open(\"./text8.txt\", \"wb\") as f:\n",
    "        f.write(corpus)\n",
    "    f.close()\n",
    "\n",
    "download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "æ¥ä¸‹æ¥ï¼ŒæŠŠä¸‹è½½çš„è¯­æ–™è¯»å–åˆ°ç¨‹åºé‡Œï¼Œå¹¶æ‰“å°å‰500ä¸ªå­—ç¬¦çœ‹çœ‹è¯­æ–™çš„æ ·å­ï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
     ]
    }
   ],
   "source": [
    "#è¯»å–text8æ•°æ®\n",
    "def load_text8():\n",
    "    with open(\"./text8.txt\", \"r\") as f:\n",
    "        corpus = f.read().strip(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    return corpus\n",
    "\n",
    "corpus = load_text8()\n",
    "\n",
    "#æ‰“å°å‰500ä¸ªå­—ç¬¦ï¼Œç®€è¦çœ‹ä¸€ä¸‹è¿™ä¸ªè¯­æ–™çš„æ ·å­\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œéœ€è¦å…ˆå¯¹è¯­æ–™è¿›è¡Œåˆ‡è¯ã€‚å¯¹äºè‹±æ–‡æ¥è¯´ï¼Œå¯ä»¥æ¯”è¾ƒç®€å•åœ°ç›´æ¥ä½¿ç”¨ç©ºæ ¼è¿›è¡Œåˆ‡è¯ï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the']\n"
     ]
    }
   ],
   "source": [
    "#å¯¹è¯­æ–™è¿›è¡Œé¢„å¤„ç†ï¼ˆåˆ†è¯ï¼‰\n",
    "def data_preprocess(corpus):\n",
    "    #ç”±äºè‹±æ–‡å•è¯å‡ºç°åœ¨å¥é¦–çš„æ—¶å€™ç»å¸¸è¦å¤§å†™ï¼Œæ‰€ä»¥æˆ‘ä»¬æŠŠæ‰€æœ‰è‹±æ–‡å­—ç¬¦éƒ½è½¬æ¢ä¸ºå°å†™ï¼Œ\n",
    "    #ä»¥ä¾¿å¯¹è¯­æ–™è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼ˆApple vs appleç­‰ï¼‰\n",
    "    corpus = corpus.strip().lower()\n",
    "    corpus = corpus.split(\" \")\n",
    "\n",
    "    return corpus\n",
    "\n",
    "corpus = data_preprocess(corpus)\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "åœ¨ç»è¿‡åˆ‡è¯åï¼Œéœ€è¦å¯¹è¯­æ–™è¿›è¡Œç»Ÿè®¡ï¼Œä¸ºæ¯ä¸ªè¯æ„é€ IDã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¯ä»¥æ ¹æ®æ¯ä¸ªè¯åœ¨è¯­æ–™ä¸­å‡ºç°çš„é¢‘æ¬¡æ„é€ IDï¼Œé¢‘æ¬¡è¶Šé«˜ï¼ŒIDè¶Šå°ï¼Œä¾¿äºå¯¹è¯å…¸è¿›è¡Œç®¡ç†ã€‚ä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#æ„é€ è¯å…¸ï¼Œç»Ÿè®¡æ¯ä¸ªè¯çš„é¢‘ç‡ï¼Œå¹¶æ ¹æ®é¢‘ç‡å°†æ¯ä¸ªè¯è½¬æ¢ä¸ºä¸€ä¸ªæ•´æ•°id\n",
    "def build_dict(corpus):\n",
    "    #é¦–å…ˆç»Ÿè®¡æ¯ä¸ªä¸åŒè¯çš„é¢‘ç‡ï¼ˆå‡ºç°çš„æ¬¡æ•°ï¼‰ï¼Œä½¿ç”¨ä¸€ä¸ªè¯å…¸è®°å½•\n",
    "    word_freq_dict = dict()\n",
    "    for word in corpus:\n",
    "        if word not in word_freq_dict:\n",
    "            word_freq_dict[word] = 0\n",
    "        word_freq_dict[word] += 1\n",
    "\n",
    "    #å°†è¿™ä¸ªè¯å…¸ä¸­çš„è¯ï¼ŒæŒ‰ç…§å‡ºç°æ¬¡æ•°æ’åºï¼Œå‡ºç°æ¬¡æ•°è¶Šé«˜ï¼Œæ’åºè¶Šé å‰\n",
    "    #ä¸€èˆ¬æ¥è¯´ï¼Œå‡ºç°é¢‘ç‡é«˜çš„é«˜é¢‘è¯å¾€å¾€æ˜¯ï¼šIï¼Œtheï¼Œyouè¿™ç§ä»£è¯ï¼Œè€Œå‡ºç°é¢‘ç‡ä½çš„è¯ï¼Œå¾€å¾€æ˜¯ä¸€äº›åè¯ï¼Œå¦‚ï¼šnlp\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    #æ„é€ 3ä¸ªä¸åŒçš„è¯å…¸ï¼Œåˆ†åˆ«å­˜å‚¨ï¼Œ\n",
    "    #æ¯ä¸ªè¯åˆ°idçš„æ˜ å°„å…³ç³»ï¼šword2id_dict\n",
    "    #æ¯ä¸ªidå‡ºç°çš„é¢‘ç‡ï¼šword2id_freq\n",
    "    #æ¯ä¸ªidåˆ°è¯å…¸æ˜ å°„å…³ç³»ï¼šid2word_dict\n",
    "    word2id_dict = dict()\n",
    "    word2id_freq = dict()\n",
    "    id2word_dict = dict()\n",
    "\n",
    "    #æŒ‰ç…§é¢‘ç‡ï¼Œä»é«˜åˆ°ä½ï¼Œå¼€å§‹éå†æ¯ä¸ªå•è¯ï¼Œå¹¶ä¸ºè¿™ä¸ªå•è¯æ„é€ ä¸€ä¸ªç‹¬ä¸€æ— äºŒçš„id\n",
    "    for word, freq in word_freq_dict:\n",
    "        curr_id = len(word2id_dict)\n",
    "        word2id_dict[word] = curr_id\n",
    "        word2id_freq[word2id_dict[word]] = freq\n",
    "        id2word_dict[curr_id] = word\n",
    "\n",
    "    return word2id_freq, word2id_dict, id2word_dict\n",
    "\n",
    "word2id_freq, word2id_dict, id2word_dict = build_dict(corpus)\n",
    "vocab_size = len(word2id_freq)\n",
    "print(\"there are totoally %d different words in the corpus\" % vocab_size)\n",
    "for _, (word, word_id) in zip(range(50), word2id_dict.items()):\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "å¾—åˆ°word2idè¯å…¸åï¼Œæˆ‘ä»¬è¿˜éœ€è¦è¿›ä¸€æ­¥å¤„ç†åŸå§‹è¯­æ–™ï¼ŒæŠŠæ¯ä¸ªè¯æ›¿æ¢æˆå¯¹åº”çš„IDï¼Œä¾¿äºç¥ç»ç½‘ç»œè¿›è¡Œå¤„ç†ï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#æŠŠè¯­æ–™è½¬æ¢ä¸ºidåºåˆ—\n",
    "def convert_corpus_to_id(corpus, word2id_dict):\n",
    "    #ä½¿ç”¨ä¸€ä¸ªå¾ªç¯ï¼Œå°†è¯­æ–™ä¸­çš„æ¯ä¸ªè¯æ›¿æ¢æˆå¯¹åº”çš„idï¼Œä»¥ä¾¿äºç¥ç»ç½‘ç»œè¿›è¡Œå¤„ç†\n",
    "    corpus = [word2id_dict[word] for word in corpus]\n",
    "    return corpus\n",
    "\n",
    "corpus = convert_corpus_to_id(corpus, word2id_dict)\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "æ¥ä¸‹æ¥ï¼Œéœ€è¦ä½¿ç”¨äºŒæ¬¡é‡‡æ ·æ³•å¤„ç†åŸå§‹æ–‡æœ¬ã€‚äºŒæ¬¡é‡‡æ ·æ³•çš„ä¸»è¦æ€æƒ³æ˜¯é™ä½é«˜é¢‘è¯åœ¨è¯­æ–™ä¸­å‡ºç°çš„é¢‘æ¬¡ï¼Œé™ä½çš„æ–¹æ³•æ˜¯éšæœºå°†é«˜é¢‘çš„è¯æŠ›å¼ƒï¼Œé¢‘ç‡è¶Šé«˜ï¼Œè¢«æŠ›å¼ƒçš„æ¦‚ç‡å°±è¶Šé«˜ï¼Œé¢‘ç‡è¶Šä½ï¼Œè¢«æŠ›å¼ƒçš„æ¦‚ç‡å°±è¶Šä½ï¼Œè¿™æ ·åƒæ ‡ç‚¹ç¬¦å·æˆ–å† è¯è¿™æ ·çš„é«˜é¢‘è¯å°±ä¼šè¢«æŠ›å¼ƒï¼Œä»è€Œä¼˜åŒ–æ•´ä¸ªè¯è¡¨çš„è¯å‘é‡è®­ç»ƒæ•ˆæœï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ä½¿ç”¨äºŒæ¬¡é‡‡æ ·ç®—æ³•ï¼ˆsubsamplingï¼‰å¤„ç†è¯­æ–™ï¼Œå¼ºåŒ–è®­ç»ƒæ•ˆæœ\n",
    "def subsampling(corpus, word2id_freq):\n",
    "    \n",
    "    #è¿™ä¸ªdiscardå‡½æ•°å†³å®šäº†ä¸€ä¸ªè¯ä¼šä¸ä¼šè¢«æ›¿æ¢ï¼Œè¿™ä¸ªå‡½æ•°æ˜¯å…·æœ‰éšæœºæ€§çš„ï¼Œæ¯æ¬¡è°ƒç”¨ç»“æœä¸åŒ\n",
    "    #å¦‚æœä¸€ä¸ªè¯çš„é¢‘ç‡å¾ˆå¤§ï¼Œé‚£ä¹ˆå®ƒè¢«é—å¼ƒçš„æ¦‚ç‡å°±å¾ˆå¤§\n",
    "    def discard(word_id):\n",
    "        return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "            1e-4 / word2id_freq[word_id] * len(corpus))\n",
    "\n",
    "    corpus = [word for word in corpus if not discard(word)]\n",
    "    return corpus\n",
    "\n",
    "corpus = subsampling(corpus, word2id_freq)\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "åœ¨å®Œæˆè¯­æ–™æ•°æ®é¢„å¤„ç†ä¹‹åï¼Œéœ€è¦æ„é€ è®­ç»ƒæ•°æ®ã€‚æ ¹æ®ä¸Šé¢çš„æè¿°ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ä¸€ä¸ªæ»‘åŠ¨çª—å£å¯¹è¯­æ–™ä»å·¦åˆ°å³æ‰«æï¼Œåœ¨æ¯ä¸ªçª—å£å†…ï¼Œä¸­å¿ƒè¯éœ€è¦é¢„æµ‹å®ƒçš„ä¸Šä¸‹æ–‡ï¼Œå¹¶å½¢æˆè®­ç»ƒæ•°æ®ã€‚\n",
    "\n",
    "åœ¨å®é™…æ“ä½œä¸­ï¼Œç”±äºè¯è¡¨å¾€å¾€å¾ˆå¤§ï¼ˆ50000ï¼Œ100000ç­‰ï¼‰ï¼Œå¯¹å¤§è¯è¡¨çš„ä¸€äº›çŸ©é˜µè¿ç®—ï¼ˆå¦‚softmaxï¼‰éœ€è¦æ¶ˆè€—å·¨å¤§çš„èµ„æºï¼Œå› æ­¤å¯ä»¥é€šè¿‡è´Ÿé‡‡æ ·çš„æ–¹å¼æ¨¡æ‹Ÿsoftmaxçš„ç»“æœï¼Œä»£ç å®ç°å¦‚ä¸‹ã€‚\n",
    "* ç»™å®šä¸€ä¸ªä¸­å¿ƒè¯å’Œä¸€ä¸ªéœ€è¦é¢„æµ‹çš„ä¸Šä¸‹æ–‡è¯ï¼ŒæŠŠè¿™ä¸ªä¸Šä¸‹æ–‡è¯ä½œä¸ºæ­£æ ·æœ¬ã€‚\n",
    "* é€šè¿‡è¯è¡¨éšæœºé‡‡æ ·çš„æ–¹å¼ï¼Œé€‰æ‹©è‹¥å¹²ä¸ªè´Ÿæ ·æœ¬ã€‚\n",
    "* æŠŠä¸€ä¸ªå¤§è§„æ¨¡åˆ†ç±»é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ª2åˆ†ç±»é—®é¢˜ï¼Œé€šè¿‡è¿™ç§æ–¹å¼ä¼˜åŒ–è®¡ç®—é€Ÿåº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#æ„é€ æ•°æ®ï¼Œå‡†å¤‡æ¨¡å‹è®­ç»ƒ\n",
    "#max_window_sizeä»£è¡¨äº†æœ€å¤§çš„window_sizeçš„å¤§å°ï¼Œç¨‹åºä¼šæ ¹æ®max_window_sizeä»å·¦åˆ°å³æ‰«ææ•´ä¸ªè¯­æ–™\n",
    "#negative_sample_numä»£è¡¨äº†å¯¹äºæ¯ä¸ªæ­£æ ·æœ¬ï¼Œæˆ‘ä»¬éœ€è¦éšæœºé‡‡æ ·å¤šå°‘è´Ÿæ ·æœ¬ç”¨äºè®­ç»ƒï¼Œ\n",
    "#ä¸€èˆ¬æ¥è¯´ï¼Œnegative_sample_numçš„å€¼è¶Šå¤§ï¼Œè®­ç»ƒæ•ˆæœè¶Šç¨³å®šï¼Œä½†æ˜¯è®­ç»ƒé€Ÿåº¦è¶Šæ…¢ã€‚ \n",
    "def build_data(corpus, word2id_dict, word2id_freq, max_window_size = 3, \n",
    "               negative_sample_num = 4):\n",
    "    \n",
    "    #ä½¿ç”¨ä¸€ä¸ªlistå­˜å‚¨å¤„ç†å¥½çš„æ•°æ®\n",
    "    dataset = []\n",
    "    center_word_idx=0\n",
    "\n",
    "    #ä»å·¦åˆ°å³ï¼Œå¼€å§‹æšä¸¾æ¯ä¸ªä¸­å¿ƒç‚¹çš„ä½ç½®\n",
    "    while center_word_idx < len(corpus):\n",
    "        #ä»¥max_window_sizeä¸ºä¸Šé™ï¼Œéšæœºé‡‡æ ·ä¸€ä¸ªwindow_sizeï¼Œè¿™æ ·ä¼šä½¿å¾—è®­ç»ƒæ›´åŠ ç¨³å®š\n",
    "        window_size = random.randint(1, max_window_size)\n",
    "        #å½“å‰çš„ä¸­å¿ƒè¯å°±æ˜¯center_word_idxæ‰€æŒ‡å‘çš„è¯ï¼Œå¯ä»¥å½“ä½œæ­£æ ·æœ¬\n",
    "        positive_word = corpus[center_word_idx]\n",
    "\n",
    "        #ä»¥å½“å‰ä¸­å¿ƒè¯ä¸ºä¸­å¿ƒï¼Œå·¦å³ä¸¤ä¾§åœ¨window_sizeå†…çš„è¯å°±æ˜¯ä¸Šä¸‹æ–‡\n",
    "        context_word_range = (max(0, center_word_idx - window_size), min(len(corpus) - 1, center_word_idx + window_size))\n",
    "        context_word_candidates = [corpus[idx] for idx in range(context_word_range[0], context_word_range[1]+1) if idx != center_word_idx]\n",
    "\n",
    "        #å¯¹äºæ¯ä¸ªæ­£æ ·æœ¬æ¥è¯´ï¼Œéšæœºé‡‡æ ·negative_sample_numä¸ªè´Ÿæ ·æœ¬ï¼Œç”¨äºè®­ç»ƒ\n",
    "        for context_word in context_word_candidates:\n",
    "            #é¦–å…ˆæŠŠï¼ˆä¸Šä¸‹æ–‡ï¼Œæ­£æ ·æœ¬ï¼Œlabel=1ï¼‰çš„ä¸‰å…ƒç»„æ•°æ®æ”¾å…¥datasetä¸­ï¼Œ\n",
    "            #è¿™é‡Œlabel=1è¡¨ç¤ºè¿™ä¸ªæ ·æœ¬æ˜¯ä¸ªæ­£æ ·æœ¬\n",
    "            dataset.append((positive_word, context_word, 1))\n",
    "\n",
    "            #å¼€å§‹è´Ÿé‡‡æ ·\n",
    "            i = 0\n",
    "            while i < negative_sample_num:\n",
    "                negative_word_candidate = random.randint(0, vocab_size-1)\n",
    "\n",
    "                if negative_word_candidate is not context_word:\n",
    "                    #æŠŠï¼ˆä¸Šä¸‹æ–‡ï¼Œè´Ÿæ ·æœ¬ï¼Œlabel=0ï¼‰çš„ä¸‰å…ƒç»„æ•°æ®æ”¾å…¥datasetä¸­ï¼Œ\n",
    "                    #è¿™é‡Œlabel=0è¡¨ç¤ºè¿™ä¸ªæ ·æœ¬æ˜¯ä¸ªè´Ÿæ ·æœ¬\n",
    "                    dataset.append((positive_word, negative_word_candidate, 0))\n",
    "                    i += 1\n",
    "        \n",
    "        center_word_idx = min(len(corpus) - 1, center_word_idx + window_size)\n",
    "        if center_word_idx == (len(corpus) - 1):\n",
    "            center_word_idx += 1\n",
    "        if center_word_idx % 100000 == 0:\n",
    "            print(center_word_idx)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = build_data(corpus, word2id_dict, word2id_freq)\n",
    "for _, (context_word, target_word, label) in zip(range(50), dataset):\n",
    "    print(\"center_word %s, target %s, label %d\" % (id2word_dict[context_word],\n",
    "                                                   id2word_dict[target_word], label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " è®­ç»ƒæ•°æ®å‡†å¤‡å¥½åï¼ŒæŠŠè®­ç»ƒæ•°æ®éƒ½ç»„è£…æˆmini-batchï¼Œå¹¶å‡†å¤‡è¾“å…¥åˆ°ç½‘ç»œä¸­è¿›è¡Œè®­ç»ƒï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#æ„é€ mini-batchï¼Œå‡†å¤‡å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒ\n",
    "#æˆ‘ä»¬å°†ä¸åŒç±»å‹çš„æ•°æ®æ”¾åˆ°ä¸åŒçš„tensoré‡Œï¼Œä¾¿äºç¥ç»ç½‘ç»œè¿›è¡Œå¤„ç†\n",
    "#å¹¶é€šè¿‡numpyçš„arrayå‡½æ•°ï¼Œæ„é€ å‡ºä¸åŒçš„tensoræ¥ï¼Œå¹¶æŠŠè¿™äº›tensoré€å…¥ç¥ç»ç½‘ç»œä¸­è¿›è¡Œè®­ç»ƒ\n",
    "def build_batch(dataset, batch_size, epoch_num):\n",
    "    \n",
    "    #center_word_batchç¼“å­˜batch_sizeä¸ªä¸­å¿ƒè¯\n",
    "    center_word_batch = []\n",
    "    #target_word_batchç¼“å­˜batch_sizeä¸ªç›®æ ‡è¯ï¼ˆå¯ä»¥æ˜¯æ­£æ ·æœ¬æˆ–è€…è´Ÿæ ·æœ¬ï¼‰\n",
    "    target_word_batch = []\n",
    "    #label_batchç¼“å­˜äº†batch_sizeä¸ª0æˆ–1çš„æ ‡ç­¾ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒ\n",
    "    label_batch = []\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        #æ¯æ¬¡å¼€å¯ä¸€ä¸ªæ–°epochä¹‹å‰ï¼Œéƒ½å¯¹æ•°æ®è¿›è¡Œä¸€æ¬¡éšæœºæ‰“ä¹±ï¼Œæé«˜è®­ç»ƒæ•ˆæœ\n",
    "        random.shuffle(dataset)\n",
    "        \n",
    "        for center_word, target_word, label in dataset:\n",
    "            #éå†datasetä¸­çš„æ¯ä¸ªæ ·æœ¬ï¼Œå¹¶å°†è¿™äº›æ•°æ®é€åˆ°ä¸åŒçš„tensoré‡Œ\n",
    "            center_word_batch.append([center_word])\n",
    "            target_word_batch.append([target_word])\n",
    "            label_batch.append(label)\n",
    "\n",
    "            #å½“æ ·æœ¬ç§¯æ”’åˆ°ä¸€ä¸ªbatch_sizeåï¼Œæˆ‘ä»¬æŠŠæ•°æ®éƒ½è¿”å›å›æ¥\n",
    "            #åœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨numpyçš„arrayå‡½æ•°æŠŠlistå°è£…æˆtensor\n",
    "            #å¹¶ä½¿ç”¨pythonçš„è¿­ä»£å™¨æœºåˆ¶ï¼Œå°†æ•°æ®yieldå‡ºæ¥\n",
    "            #ä½¿ç”¨è¿­ä»£å™¨çš„å¥½å¤„æ˜¯å¯ä»¥èŠ‚çœå†…å­˜\n",
    "            if len(center_word_batch) == batch_size:\n",
    "                yield np.array(center_word_batch).astype(\"int64\"), \\\n",
    "                    np.array(target_word_batch).astype(\"int64\"), \\\n",
    "                    np.array(label_batch).astype(\"float32\")\n",
    "                center_word_batch = []\n",
    "                target_word_batch = []\n",
    "                label_batch = []\n",
    "\n",
    "    if len(center_word_batch) > 0:\n",
    "        yield np.array(center_word_batch).astype(\"int64\"), \\\n",
    "            np.array(target_word_batch).astype(\"int64\"), \\\n",
    "            np.array(label_batch).astype(\"float32\")\n",
    "\n",
    "# for _, batch in zip(range(10), build_batch(dataset, 128, 3)):\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ç½‘ç»œå®šä¹‰\n",
    "\n",
    "å®šä¹‰cbowçš„ç½‘ç»œç»“æ„ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒã€‚åœ¨é£æ¡¨åŠ¨æ€å›¾ä¸­ï¼Œå¯¹äºä»»æ„ç½‘ç»œï¼Œéƒ½éœ€è¦å®šä¹‰ä¸€ä¸ªç»§æ‰¿è‡ªpaddle.nn.Layerçš„ç±»æ¥æ­å»ºç½‘ç»œç»“æ„ã€å‚æ•°ç­‰æ•°æ®çš„å£°æ˜ã€‚åŒæ—¶éœ€è¦åœ¨forwardå‡½æ•°ä¸­å®šä¹‰ç½‘ç»œçš„è®¡ç®—é€»è¾‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä»…éœ€è¦å®šä¹‰ç½‘ç»œçš„å‰å‘è®¡ç®—é€»è¾‘ï¼Œé£æ¡¨ä¼šè‡ªåŠ¨å®Œæˆç¥ç»ç½‘ç»œçš„åå‘è®¡ç®—ï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#å®šä¹‰cbowè®­ç»ƒç½‘ç»œç»“æ„\n",
    "#è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯paddlepaddleçš„2.0.0ç‰ˆæœ¬\n",
    "#ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨ä½¿ç”¨nnè®­ç»ƒçš„æ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ä¸€ä¸ªç±»æ¥å®šä¹‰ç½‘ç»œç»“æ„ï¼Œè¿™ä¸ªç±»ç»§æ‰¿äº†paddle.nn.Layer\n",
    "class CBOW(paddle.nn.Layer):\n",
    "    def __init__(self, vocab_size, embedding_size, init_scale=0.1):\n",
    "        #vocab_sizeå®šä¹‰äº†è¿™ä¸ªCBOWè¿™ä¸ªæ¨¡å‹çš„è¯è¡¨å¤§å°\n",
    "        #embedding_sizeå®šä¹‰äº†è¯å‘é‡çš„ç»´åº¦æ˜¯å¤šå°‘\n",
    "        #init_scaleå®šä¹‰äº†è¯å‘é‡åˆå§‹åŒ–çš„èŒƒå›´ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œæ¯”è¾ƒå°çš„åˆå§‹åŒ–èŒƒå›´æœ‰åŠ©äºæ¨¡å‹è®­ç»ƒ\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        #ä½¿ç”¨paddle.nnæä¾›çš„Embeddingå‡½æ•°ï¼Œæ„é€ ä¸€ä¸ªè¯å‘é‡å‚æ•°\n",
    "        #è¿™ä¸ªå‚æ•°çš„å¤§å°ä¸ºï¼šself.vocab_size, self.embedding_size\n",
    "        #è¿™ä¸ªå‚æ•°çš„åç§°ä¸ºï¼šembedding_para\n",
    "        #è¿™ä¸ªå‚æ•°çš„åˆå§‹åŒ–æ–¹å¼ä¸ºåœ¨[-init_scale, init_scale]åŒºé—´è¿›è¡Œå‡åŒ€é‡‡æ ·\n",
    "        self.embedding = paddle.nn.Embedding(\n",
    "            self.vocab_size, \n",
    "            self.embedding_size,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                name='embedding_para',\n",
    "                initializer=paddle.nn.initializer.Uniform(\n",
    "                    low=-0.5/embedding_size, high=0.5/embedding_size)))\n",
    "\n",
    "        #ä½¿ç”¨paddle.nnæä¾›çš„Embeddingå‡½æ•°ï¼Œæ„é€ å¦å¤–ä¸€ä¸ªè¯å‘é‡å‚æ•°\n",
    "        #è¿™ä¸ªå‚æ•°çš„å¤§å°ä¸ºï¼šself.vocab_size, self.embedding_size\n",
    "        #è¿™ä¸ªå‚æ•°çš„åç§°ä¸ºï¼šembedding_para_out\n",
    "        #è¿™ä¸ªå‚æ•°çš„åˆå§‹åŒ–æ–¹å¼ä¸ºåœ¨[-init_scale, init_scale]åŒºé—´è¿›è¡Œå‡åŒ€é‡‡æ ·\n",
    "        #è·Ÿä¸Šé¢ä¸åŒçš„æ˜¯ï¼Œè¿™ä¸ªå‚æ•°çš„åç§°è·Ÿä¸Šé¢ä¸åŒï¼Œå› æ­¤ï¼Œ\n",
    "        #embedding_para_outå’Œembedding_paraè™½ç„¶æœ‰ç›¸åŒçš„shapeï¼Œä½†æ˜¯æƒé‡ä¸å…±äº«\n",
    "        self.embedding_out = paddle.nn.Embedding(\n",
    "            self.vocab_size, \n",
    "            self.embedding_size,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                name='embedding_out_para',\n",
    "                initializer=paddle.nn.initializer.Uniform(\n",
    "                    low=-0.5/embedding_size, high=0.5/embedding_size)))\n",
    "\n",
    "    #å®šä¹‰ç½‘ç»œçš„å‰å‘è®¡ç®—é€»è¾‘\n",
    "    #center_wordsæ˜¯ä¸€ä¸ªtensorï¼ˆmini-batchï¼‰ï¼Œè¡¨ç¤ºä¸­å¿ƒè¯\n",
    "    #target_wordsæ˜¯ä¸€ä¸ªtensorï¼ˆmini-batchï¼‰ï¼Œè¡¨ç¤ºç›®æ ‡è¯\n",
    "    #labelæ˜¯ä¸€ä¸ªtensorï¼ˆmini-batchï¼‰ï¼Œè¡¨ç¤ºè¿™ä¸ªè¯æ˜¯æ­£æ ·æœ¬è¿˜æ˜¯è´Ÿæ ·æœ¬ï¼ˆç”¨0æˆ–1è¡¨ç¤ºï¼‰\n",
    "    #ç”¨äºåœ¨è®­ç»ƒä¸­è®¡ç®—è¿™ä¸ªtensorä¸­å¯¹åº”è¯çš„åŒä¹‰è¯ï¼Œç”¨äºè§‚å¯Ÿæ¨¡å‹çš„è®­ç»ƒæ•ˆæœ\n",
    "    def forward(self, center_words, target_words, label):\n",
    "        #é¦–å…ˆï¼Œé€šè¿‡embedding_paraï¼ˆself.embeddingï¼‰å‚æ•°ï¼Œå°†mini-batchä¸­çš„è¯è½¬æ¢ä¸ºè¯å‘é‡\n",
    "        #è¿™é‡Œcenter_wordså’Œeval_words_embæŸ¥è¯¢çš„æ˜¯ä¸€ä¸ªç›¸åŒçš„å‚æ•°\n",
    "        #è€Œtarget_words_embæŸ¥è¯¢çš„æ˜¯å¦ä¸€ä¸ªå‚æ•°\n",
    "        center_words_emb = self.embedding(center_words)\n",
    "        target_words_emb = self.embedding_out(target_words)\n",
    "\n",
    "        #center_words_emb = [batch_size, embedding_size]\n",
    "        #target_words_emb = [batch_size, embedding_size]\n",
    "        #æˆ‘ä»¬é€šè¿‡ç‚¹ä¹˜çš„æ–¹å¼è®¡ç®—ä¸­å¿ƒè¯åˆ°ç›®æ ‡è¯çš„è¾“å‡ºæ¦‚ç‡ï¼Œå¹¶é€šè¿‡sigmoidå‡½æ•°ä¼°è®¡è¿™ä¸ªè¯æ˜¯æ­£æ ·æœ¬è¿˜æ˜¯è´Ÿæ ·æœ¬çš„æ¦‚ç‡ã€‚\n",
    "        word_sim = paddle.multiply(center_words_emb, target_words_emb)\n",
    "        word_sim = paddle.sum(word_sim, axis = -1)\n",
    "        word_sim = paddle.reshape(word_sim, shape=[-1])\n",
    "        pred = paddle.nn.functional.sigmoid(word_sim)\n",
    "\n",
    "        #é€šè¿‡ä¼°è®¡çš„è¾“å‡ºæ¦‚ç‡å®šä¹‰æŸå¤±å‡½æ•°ï¼Œæ³¨æ„æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯binary_cross_entropyå‡½æ•°\n",
    "        #å°†sigmoidè®¡ç®—å’Œcross entropyåˆå¹¶æˆä¸€æ­¥è®¡ç®—å¯ä»¥æ›´å¥½çš„ä¼˜åŒ–ï¼Œæ‰€ä»¥è¾“å…¥çš„æ˜¯word_simï¼Œè€Œä¸æ˜¯pred\n",
    "        \n",
    "        loss = paddle.nn.functional.binary_cross_entropy(paddle.nn.functional.sigmoid(word_sim), label)\n",
    "        loss = paddle.mean(loss)\n",
    "\n",
    "        #è¿”å›å‰å‘è®¡ç®—çš„ç»“æœï¼Œé£æ¡¨ä¼šé€šè¿‡backwardå‡½æ•°è‡ªåŠ¨è®¡ç®—å‡ºåå‘ç»“æœã€‚\n",
    "        return pred, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ç½‘ç»œè®­ç»ƒ\n",
    "\n",
    "å®Œæˆç½‘ç»œå®šä¹‰åï¼Œå°±å¯ä»¥å¯åŠ¨æ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬å®šä¹‰æ¯éš”100æ­¥æ‰“å°ä¸€æ¬¡Lossï¼Œä»¥ç¡®ä¿å½“å‰çš„ç½‘ç»œæ˜¯æ­£å¸¸æ”¶æ•›çš„ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ¯éš”1000æ­¥è§‚å¯Ÿä¸€ä¸‹cbowè®¡ç®—å‡ºæ¥çš„åŒä¹‰è¯ï¼ˆä½¿ç”¨ embeddingçš„ä¹˜ç§¯ï¼‰ï¼Œå¯è§†åŒ–ç½‘ç»œè®­ç»ƒæ•ˆæœï¼Œä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#å¼€å§‹è®­ç»ƒï¼Œå®šä¹‰ä¸€äº›è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦ä½¿ç”¨çš„è¶…å‚æ•°\n",
    "batch_size = 512\n",
    "epoch_num = 3\n",
    "embedding_size = 200\n",
    "step = 0\n",
    "learning_rate = 0.001\n",
    "\n",
    "#å®šä¹‰ä¸€ä¸ªä½¿ç”¨word-embeddingè®¡ç®—cosçš„å‡½æ•°\n",
    "def get_cos(query1_token, query2_token, embed):\n",
    "    W = embed\n",
    "    x = W[word2id_dict[query1_token]]\n",
    "    y = W[word2id_dict[query2_token]]\n",
    "    cos = np.dot(x, y) / np.sqrt(np.sum(y * y) * np.sum(x * x) + 1e-9)\n",
    "    flat = cos.flatten()\n",
    "    print(\"å•è¯1 %s å’Œå•è¯2 %s çš„cosç»“æœä¸º %f\" %(query1_token, query2_token, cos))\n",
    "\n",
    "\n",
    "#é€šè¿‡æˆ‘ä»¬å®šä¹‰çš„CBOWç±»ï¼Œæ¥æ„é€ ä¸€ä¸ªcbowæ¨¡å‹ç½‘ç»œ\n",
    "skip_gram_model = CBOW(vocab_size, embedding_size)\n",
    "#æ„é€ è®­ç»ƒè¿™ä¸ªç½‘ç»œçš„ä¼˜åŒ–å™¨\n",
    "adam = paddle.optimizer.Adam(learning_rate=learning_rate, parameters = skip_gram_model.parameters())\n",
    "\n",
    "#ä½¿ç”¨build_batchå‡½æ•°ï¼Œä»¥mini-batchä¸ºå•ä½ï¼Œéå†è®­ç»ƒæ•°æ®ï¼Œå¹¶è®­ç»ƒç½‘ç»œ\n",
    "for center_words, target_words, label in build_batch(\n",
    "    dataset, batch_size, epoch_num):\n",
    "    #ä½¿ç”¨paddle.to_tensorå‡½æ•°ï¼Œå°†ä¸€ä¸ªnumpyçš„tensorï¼Œè½¬æ¢ä¸ºé£æ¡¨å¯è®¡ç®—çš„tensor\n",
    "    center_words_var = paddle.to_tensor(center_words)\n",
    "    target_words_var = paddle.to_tensor(target_words)\n",
    "    label_var = paddle.to_tensor(label)\n",
    "\n",
    "    #å°†è½¬æ¢åçš„tensoré€å…¥é£æ¡¨ä¸­ï¼Œè¿›è¡Œä¸€æ¬¡å‰å‘è®¡ç®—ï¼Œå¹¶å¾—åˆ°è®¡ç®—ç»“æœ\n",
    "    pred, loss = skip_gram_model(\n",
    "        center_words_var, target_words_var, label_var)\n",
    "\n",
    "    #é€šè¿‡backwardå‡½æ•°ï¼Œè®©ç¨‹åºè‡ªåŠ¨å®Œæˆåå‘è®¡ç®—\n",
    "    loss.backward()\n",
    "    #é€šè¿‡minimizeå‡½æ•°ï¼Œè®©ç¨‹åºæ ¹æ®lossï¼Œå®Œæˆä¸€æ­¥å¯¹å‚æ•°çš„ä¼˜åŒ–æ›´æ–°\n",
    "    adam.minimize(loss)\n",
    "    #ä½¿ç”¨clear_gradientså‡½æ•°æ¸…ç©ºæ¨¡å‹ä¸­çš„æ¢¯åº¦ï¼Œä»¥ä¾¿äºä¸‹ä¸€ä¸ªmini-batchè¿›è¡Œæ›´æ–°\n",
    "    skip_gram_model.clear_gradients()\n",
    "\n",
    "    #æ¯ç»è¿‡100ä¸ªmini-batchï¼Œæ‰“å°ä¸€æ¬¡å½“å‰çš„lossï¼Œçœ‹çœ‹lossæ˜¯å¦åœ¨ç¨³å®šä¸‹é™\n",
    "    step += 1\n",
    "    if step % 100 == 0:\n",
    "        print(\"step %d, loss %.3f\" % (step, loss.numpy()[0]))\n",
    "\n",
    "    #ç»è¿‡10000ä¸ªmini-batchï¼Œæ‰“å°ä¸€æ¬¡æ¨¡å‹å¯¹eval_wordsä¸­çš„10ä¸ªè¯è®¡ç®—çš„åŒä¹‰è¯\n",
    "    #è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨è¯å’Œè¯ä¹‹é—´çš„å‘é‡ç‚¹ç§¯ä½œä¸ºè¡¡é‡ç›¸ä¼¼åº¦çš„æ–¹æ³•\n",
    "    #æˆ‘ä»¬åªæ‰“å°äº†5ä¸ªæœ€ç›¸ä¼¼çš„è¯\n",
    "    if step % 2000 == 0:\n",
    "        embedding_matrix = skip_gram_model.embedding.weight.numpy()\n",
    "        np.save(\"./embedding\", embedding_matrix)\n",
    "        get_cos(\"king\",\"queen\",embedding_matrix)\n",
    "        get_cos(\"she\",\"her\",embedding_matrix)\n",
    "        get_cos(\"topic\",\"theme\",embedding_matrix)\n",
    "        get_cos(\"woman\",\"game\",embedding_matrix)\n",
    "        get_cos(\"one\",\"name\",embedding_matrix)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ä»æ‰“å°ç»“æœå¯ä»¥çœ‹åˆ°ï¼Œç»è¿‡ä¸€å®šæ­¥éª¤çš„è®­ç»ƒï¼ŒLossé€æ¸ä¸‹é™å¹¶è¶‹äºç¨³å®šã€‚\n",
    "### ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—è¯„ä»·è¯å‘é‡ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#å®šä¹‰ä¸€ä¸ªä½¿ç”¨word-embeddingè®¡ç®—cosçš„å‡½æ•°\n",
    "def get_cos(query1_token, query2_token, embed):\n",
    "    W = embed\n",
    "    x = W[word2id_dict[query1_token]]\n",
    "    y = W[word2id_dict[query2_token]]\n",
    "    cos = np.dot(x, y) / np.sqrt(np.sum(y * y) * np.sum(x * x) + 1e-9)\n",
    "    flat = cos.flatten()\n",
    "    print(\"å•è¯1 %s å’Œå•è¯2 %s çš„cosç»“æœä¸º %f\" %(query1_token, query2_token, cos) )\n",
    "\n",
    "embedding_matrix = np.load('embedding.npy') \n",
    "get_cos(\"king\",\"queen\",embedding_matrix)\n",
    "get_cos(\"she\",\"her\",embedding_matrix)\n",
    "get_cos(\"topic\",\"theme\",embedding_matrix)\n",
    "get_cos(\"woman\",\"game\",embedding_matrix)\n",
    "get_cos(\"one\",\"name\",embedding_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 应用BERT模型做短文本情绪分类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0rc7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#导入相关的模块\r\n",
    "import paddle\r\n",
    "import paddlenlp as ppnlp\r\n",
    "from paddlenlp.data import Stack, Pad, Tuple\r\n",
    "import paddle.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "from functools import partial #partial()函数可以用来固定某些参数值，并返回一个新的callable对象\r\n",
    "ppnlp.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 数据导入\n",
    "\n",
    "数据集为公开中文情感分析数据集ChnSenticorp。使用PaddleNLP的.datasets.ChnSentiCorp.get_datasets方法即可以加载该数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-05 14:56:12,675 - INFO - unique_endpoints {''}\n",
      "2021-06-05 14:56:12,676 - INFO - Downloading chnsenticorp.tar.gz from https://bj.bcebos.com/paddlehub-dataset/chnsenticorp.tar.gz\n",
      "100%|██████████| 1713/1713 [00:00<00:00, 14720.96it/s]\n",
      "2021-06-05 14:56:12,952 - INFO - File /home/aistudio/.paddlenlp/datasets/chnsenticorp.tar.gz md5 checking...\n",
      "2021-06-05 14:56:12,957 - INFO - Decompressing /home/aistudio/.paddlenlp/datasets/chnsenticorp.tar.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据：[['选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', '1']]\n",
      "\n",
      "验证集数据:[['這間酒店環境和服務態度亦算不錯,但房間空間太小~~不宣容納太大件行李~~且房間格調還可以~~ 中餐廳的廣東點心不太好吃~~要改善之~~~~但算價錢平宜~~可接受~~ 西餐廳格調都很好~~但吃的味道一般且令人等得太耐了~~要改善之~~', '1']]\n",
      "\n",
      "测试集数据:[['这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般', '1']]\n",
      "\n",
      "训练集样本个数:9600\n",
      "验证集样本个数:1200\n",
      "测试集样本个数:1200\n"
     ]
    }
   ],
   "source": [
    "#采用paddlenlp内置的ChnSentiCorp语料，该语料主要可以用来做情感分类。训练集用来训练模型，验证集用来选择模型，测试集用来评估模型泛化性能。\r\n",
    "train_ds, dev_ds, test_ds = ppnlp.datasets.ChnSentiCorp.get_datasets(['train','dev','test'])\r\n",
    "\r\n",
    "#获得标签列表\r\n",
    "label_list = train_ds.get_labels()\r\n",
    "\r\n",
    "#看看数据长什么样子，分别打印训练集、验证集、测试集的前3条数据。\r\n",
    "print(\"训练集数据：{}\\n\".format(train_ds[0:1]))\r\n",
    "print(\"验证集数据:{}\\n\".format(dev_ds[0:1]))\r\n",
    "print(\"测试集数据:{}\\n\".format(test_ds[0:1]))\r\n",
    "\r\n",
    "print(\"训练集样本个数:{}\".format(len(train_ds)))\r\n",
    "print(\"验证集样本个数:{}\".format(len(dev_ds)))\r\n",
    "print(\"测试集样本个数:{}\".format(len(test_ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-05 14:56:13,076] [    INFO] - Downloading bert-base-chinese-vocab.txt from https://paddle-hapi.bj.bcebos.com/models/bert/bert-base-chinese-vocab.txt\n",
      "100%|██████████| 107/107 [00:00<00:00, 4657.82it/s]\n"
     ]
    }
   ],
   "source": [
    "#调用ppnlp.transformers.BertTokenizer进行数据处理，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。\r\n",
    "tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(\"bert-base-chinese\")\r\n",
    "\r\n",
    "#数据预处理\r\n",
    "def convert_example(example,tokenizer,label_list,max_seq_length=256,is_test=False):\r\n",
    "    if is_test:\r\n",
    "        text = example\r\n",
    "    else:\r\n",
    "        text, label = example\r\n",
    "    #tokenizer.encode方法能够完成切分token，映射token ID以及拼接特殊token\r\n",
    "    encoded_inputs = tokenizer.encode(text=text, max_seq_len=max_seq_length)\r\n",
    "    # print('===================')\r\n",
    "    # print(encoded_inputs)\r\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\r\n",
    "    segment_ids = encoded_inputs[\"token_type_ids\"]\r\n",
    "\r\n",
    "    if not is_test:\r\n",
    "        label_map = {}\r\n",
    "        for (i, l) in enumerate(label_list):\r\n",
    "            label_map[l] = i\r\n",
    "\r\n",
    "        label = label_map[label]\r\n",
    "        label = np.array([label], dtype=\"int64\")\r\n",
    "        return input_ids, segment_ids, label\r\n",
    "    else:\r\n",
    "        return input_ids, segment_ids\r\n",
    "\r\n",
    "#数据迭代器构造方法\r\n",
    "def create_dataloader(dataset, trans_fn=None, mode='train', batch_size=1, use_gpu=False, pad_token_id=0, batchify_fn=None):\r\n",
    "    if trans_fn:\r\n",
    "        dataset = dataset.apply(trans_fn, lazy=True)\r\n",
    "\r\n",
    "    if mode == 'train' and use_gpu:\r\n",
    "        sampler = paddle.io.DistributedBatchSampler(dataset=dataset, batch_size=batch_size, shuffle=True)\r\n",
    "    else:\r\n",
    "        shuffle = True if mode == 'train' else False #如果不是训练集，则不打乱顺序\r\n",
    "        sampler = paddle.io.BatchSampler(dataset=dataset, batch_size=batch_size, shuffle=shuffle) #生成一个取样器\r\n",
    "    dataloader = paddle.io.DataLoader(dataset, batch_sampler=sampler, return_list=True, collate_fn=batchify_fn)\r\n",
    "    return dataloader\r\n",
    "\r\n",
    "#使用partial()来固定convert_example函数的tokenizer, label_list, max_seq_length, is_test等参数值\r\n",
    "trans_fn = partial(convert_example, tokenizer=tokenizer, label_list=label_list, max_seq_length=128, is_test=False)\r\n",
    "batchify_fn = lambda samples, fn=Tuple(Pad(axis=0,pad_val=tokenizer.pad_token_id), Pad(axis=0, pad_val=tokenizer.pad_token_id), Stack(dtype=\"int64\")):[data for data in fn(samples)]\r\n",
    "#训练集迭代器\r\n",
    "train_loader = create_dataloader(train_ds, mode='train', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)\r\n",
    "#验证集迭代器\r\n",
    "\r\n",
    "dev_loader = create_dataloader(dev_ds, mode='dev', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)\r\n",
    "#测试集迭代器\r\n",
    "test_loader = create_dataloader(test_ds, mode='test', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. BERT预训练模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-05 14:56:13,264] [    INFO] - Downloading http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-base-chinese.pdparams and saved to /home/aistudio/.paddlenlp/models/bert-base-chinese\n",
      "[2021-06-05 14:56:13,266] [    INFO] - Downloading bert-base-chinese.pdparams from http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-base-chinese.pdparams\n",
      "100%|██████████| 696494/696494 [00:15<00:00, 45344.36it/s]\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1303: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1303: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "#加载预训练模型Bert用于文本分类任务的Fine-tune网络BertForSequenceClassification, 它在BERT模型后接了一个全连接层进行分类。\r\n",
    "#由于本任务中的情感分类是二分类问题，设定num_classes为2\r\n",
    "model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#设置训练超参数\r\n",
    "\r\n",
    "#学习率\r\n",
    "learning_rate = 1e-5 \r\n",
    "#训练轮次\r\n",
    "epochs = 8\r\n",
    "#学习率预热比率\r\n",
    "warmup_proption = 0.1\r\n",
    "#权重衰减系数\r\n",
    "weight_decay = 0.01\r\n",
    "\r\n",
    "num_training_steps = len(train_loader) * epochs\r\n",
    "num_warmup_steps = int(warmup_proption * num_training_steps)\r\n",
    "\r\n",
    "def get_lr_factor(current_step):\r\n",
    "    if current_step < num_warmup_steps:\r\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\r\n",
    "    else:\r\n",
    "        return max(0.0,\r\n",
    "                    float(num_training_steps - current_step) /\r\n",
    "                    float(max(1, num_training_steps - num_warmup_steps)))\r\n",
    "#学习率调度器\r\n",
    "lr_scheduler = paddle.optimizer.lr.LambdaDecay(learning_rate, lr_lambda=lambda current_step: get_lr_factor(current_step))\r\n",
    "\r\n",
    "#优化器\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=lr_scheduler,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    weight_decay=weight_decay,\r\n",
    "    apply_decay_param_fun=lambda x: x in [\r\n",
    "        p.name for n, p in model.named_parameters()\r\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "    ])\r\n",
    "\r\n",
    "#损失函数\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\r\n",
    "#评估函数\r\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#评估函数\r\n",
    "def evaluate(model, criterion, metric, data_loader):\r\n",
    "    model.eval()\r\n",
    "    metric.reset()\r\n",
    "    losses = []\r\n",
    "    for batch in data_loader:\r\n",
    "        input_ids, segment_ids, labels = batch\r\n",
    "        logits = model(input_ids, segment_ids)\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "        losses.append(loss.numpy())\r\n",
    "        correct = metric.compute(logits, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        accu = metric.accumulate()\r\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\r\n",
    "    model.train()\r\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#开始训练\r\n",
    "global_step = 0\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_loader): #从训练数据迭代器中取数据\r\n",
    "        # print(batch)\r\n",
    "        input_ids, segment_ids, labels = batch\r\n",
    "        logits = model(input_ids, segment_ids)\r\n",
    "        loss = criterion(logits, labels) #计算损失\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        correct = metric.compute(probs, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        acc = metric.accumulate()\r\n",
    "\r\n",
    "        global_step += 1\r\n",
    "        if global_step % 50 == 0 :\r\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        lr_scheduler.step()\r\n",
    "        optimizer.clear_gradients()\r\n",
    "    evaluate(model, criterion, metric, dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model, data, tokenizer, label_map, batch_size=1):\r\n",
    "    examples = []\r\n",
    "    for text in data:\r\n",
    "        input_ids, segment_ids = convert_example(text, tokenizer, label_list=label_map.values(),  max_seq_length=128, is_test=True)\r\n",
    "        examples.append((input_ids, segment_ids))\r\n",
    "\r\n",
    "    batchify_fn = lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id), Pad(axis=0, pad_val=tokenizer.pad_token_id)): fn(samples)\r\n",
    "    batches = []\r\n",
    "    one_batch = []\r\n",
    "    for example in examples:\r\n",
    "        one_batch.append(example)\r\n",
    "        if len(one_batch) == batch_size:\r\n",
    "            batches.append(one_batch)\r\n",
    "            one_batch = []\r\n",
    "    if one_batch:\r\n",
    "        batches.append(one_batch)\r\n",
    "\r\n",
    "    results = []\r\n",
    "    model.eval()\r\n",
    "    for batch in batches:\r\n",
    "        input_ids, segment_ids = batchify_fn(batch)\r\n",
    "        input_ids = paddle.to_tensor(input_ids)\r\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\r\n",
    "        logits = model(input_ids, segment_ids)\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\r\n",
    "        idx = idx.tolist()\r\n",
    "        labels = [label_map[i] for i in idx]\r\n",
    "        results.extend(labels)\r\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = ['这个商品虽然看着样式挺好看的，但是不耐用。', '这个老师讲课水平挺高的。']\r\n",
    "label_map = {0: '负向情绪', 1: '正向情绪'}\r\n",
    "\r\n",
    "predictions = predict(model, data, tokenizer, label_map, batch_size=32)\r\n",
    "for idx, text in enumerate(data):\r\n",
    "    print('预测文本: {} \\n情绪标签: {}'.format(text, predictions[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
